<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[使用系统命令进入docker容器的命名空间]]></title>
    <url>%2F2019%2F12%2F03%2F%E4%BD%BF%E7%94%A8%E7%B3%BB%E7%BB%9F%E5%91%BD%E4%BB%A4%E8%BF%9B%E5%85%A5docker%E5%AE%B9%E5%99%A8%E7%9A%84%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4%2F</url>
    <content type="text"><![CDATA[前言我们都知道docker容器化技术使用了内核提供的namespace技术做资源的隔离,cgroup技术做资源的限制,但一直有一个疑惑我们docker的底层是怎么实现的,我们能不能进入docker创建的namespace. 经过一段时间的研究小有收获,此篇博客主要讲namespace,下篇讲cgroup. 查看网络名称空间ip命令给我们提供了操作网络名称空间空间的能力,我们可以使用ip netns查看、创建、操作、进入网络名称空间 但ip命令并不能直接进入docker创建的网络名称空间这是为什么? 下面我们做个实验 我们可以看见ip netns命令创建网络名称空间后在/var/run/netns/目录下下创建了一个文件 而docker创建网络名称空间的时候把描述文件放在了这里 现在我们把/var/run/docker/netns/中描述文件软连接到/var/run/netns/下 现在如何知道这个网络名称空间是那个容器的呢? 剩下的就可以使用ip netns命令随便折腾了! 所有名称空间上面的命令可以方便的对网络名称空间进行操作,但是我想进入其它命名空间怎么办呢? 我们可以使用lsns命令 查看进程所在名称空间我们先获取docker容器的进程pid 关于这个进程所在namespace可以在这个路径进程查看 进入哪我们怎么才可以进入上述所说的命名空间呢? 下面是nscenter大显身手的时候了! 通过PID进程号 通过描述符 注意:必须加sudo执行,参数细节请自行查看man手册]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手动添加docker容器到指定的网桥]]></title>
    <url>%2F2019%2F09%2F21%2F%E6%89%8B%E5%8A%A8%E6%B7%BB%E5%8A%A0docker%E5%AE%B9%E5%99%A8%E5%88%B0%E6%8C%87%E5%AE%9A%E7%9A%84%E7%BD%91%E6%A1%A5%2F</url>
    <content type="text"><![CDATA[1. 需求将docker容器加入linux内核原生支持的虚拟网桥实现dockerIP于宿主机具有相同网断 2. 原理2.1 实现原理创建虚拟网桥,创建步骤同安装KVM时创建的方案这里不在赘述,linux虚拟网桥相当于一个二层交换机 创建虚拟网卡对,将其中一半加入虚拟网桥,另一半加入docker容器所在的命名空间 激活docker容器中的哪个网卡,并为其分配ip 2.2 难点如何操作docker容器所在的网络命名空间: 方法一: 12345mkdir -p /var/run/netnsdocker inspect 容器ID|grep SandboxKey# "SandboxKey": "/var/run/docker/netns/6cca981676a4",ln -sv /var/run/docker/netns/6cca981676a4 /var/run/netns/6cca981676a4ip netns exec 6cca981676a4 ip a 方法二: 12345mkdir -p /var/run/netnsdocker inspect f7caddbf88fc|grep -i Pid# "Pid": 5132,ln -sv /proc/5132/ns/net /var/run/netns/6cca981676a4ip netns exec 6cca981676a4 ip a 3. 实现在这里我们使用一个第三方封装的工具 3.1. 安装依赖1yum install -y net-tools iproute2 bridge-utils git curl 3.2. 安装工具12wget https://raw.githubusercontent.com/jpetazzo/pipework/master/pipework -O /usr/bin/pipeworkchmod +x /usr/bin/pipework 3.3. 执行12345pipework br0 testweb1 192.168.1.3/24@192.168.1.1# br0是要添加到的目标网桥# testweb1是容器的名字# 192.168.1.3/24是为容器手动分配的IP# 192.168.1.1是容器的网关 3.4. 参数解释参考:https://github.com/jpetazzo/pipework]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设置kubeadm自签证书为100年]]></title>
    <url>%2F2019%2F09%2F11%2F%E8%AE%BE%E7%BD%AEkubeadm%E8%87%AA%E7%AD%BE%E8%AF%81%E4%B9%A6%E4%B8%BA100%E5%B9%B4%2F</url>
    <content type="text"><![CDATA[最近在更新k8s集群时留意到使用kubeadm创建的集群默认CA的有效期为10年,使用 kubeadm alpha certs renew all 更新的证书有效期为一年,本文将通过修改kubernetes源码并重新编译方式调整CA证书有效期为100年,kubeadm更新证书有效期为10年. 1. 安装kubeadm,kubelet,kubectl1yum install -y kubectl kubelet kubadm 2. 获取kubeadm版本信息12~]# kubeadm versionkubeadm version: &amp;version.Info&#123;Major:"1", Minor:"15", GitVersion:"v1.15.3", GitCommit:"2d3c76f9091b6bec110a5e63777c332469e0cba2", GitTreeState:"clean", BuildDate:"2019-08-19T11:11:18Z", GoVersion:"go1.12.9", Compiler:"gc", Platform:"linux/amd64"&#125; 3. 安装对应版本golang3.1. 下载golang安装包1wget https://dl.google.com/go/go1.12.9.linux-amd64.tar.gz 3.2. 解压安装1tar xvf go1.12.9.linux-amd64.tar.gz -C /usr/local/ 3.3. 设置环境变量123456mkdir -p /home/gocat &gt; /etc/profile.d/go.sh &lt;&lt;EOFexport GOPATH=/home/goexport PATH=/usr/local/go/bin:$PATHEOFsource /etc/profile.d/go.sh 4. 编译kubeadm4.1. 下载对应kubernetes版本源码12345wget https://dl.k8s.io/v1.15.3/kubernetes-src.tar.gz# 此地址需要科学上网mkdir -p $GOPATH/src/k8s.io/kubernetestar xvf -C $GOPATH/src/k8s.io/kubernetes kubernetes-src.tar.gz 4.2. 修改源码1234cd $GOPATH/src/k8s.io/kubernetescd vendor/k8s.io/client-go/util/certvim cert.go# 此处修改的是自签CA的有效期 1234cd $GOPATH/src/k8s.io/kubernetescd cmd/kubeadm/app/constants/vim constants.go# 此处修改的是kubeadm生成的有效期 4.3. 编译123yum install -y rsynccd $GOPATH/src/k8s.io/kubernetesKUBE_BUILD_PLATFORMS=linux/amd64 make all WHAT=cmd/kubeadm GOFLAGS=-v GOGCFLAGS="-N -l" 4.4. 安装12345_output/bin/kubeadm version# kubeadm version: &amp;version.Info&#123;Major:"1", Minor:"15", GitVersion:"v1.15.3", GitCommit:"2d3c76f9091b6bec110a5e63777c332469e0cba2", GitTreeState:"archive", BuildDate:"2019-09-11T02:54:19Z", GoVersion:"go1.12.9", Compiler:"gc", Platform:"linux/amd64"&#125;kubeadm version# version: &amp;version.Info&#123;Major:"1", Minor:"15", GitVersion:"v1.15.3", GitCommit:"2d3c76f9091b6bec110a5e63777c332469e0cba2", GitTreeState:"clean", BuildDate:"2019-08-19T11:11:18Z", GoVersion:"go1.12.9", Compiler:"gc", Platform:"linux/amd64"&#125;/usr/bin/cp _output/bin/kubeadm /bin/kubeadm 5. 测试效果12345678910kubeadm init --config=kubeadm-config.yaml --upload-certs cd /etc/kubernetes/pki/openssl x509 -in ca.crt -noout -text|grep 'Not After'# Not After : Aug 17 14:08:10 2119 GMTopenssl x509 -in front-proxy-ca.crt -noout -text|grep 'Not After' # Not After : Aug 17 14:08:09 2119 GMTopenssl x509 -in etcd/ca.crt -noout -text|grep 'Not After'# Not After : Aug 17 14:08:09 2119 GMTkubeadm alpha certs renew all # Not After : Sep 8 03:07:05 2029 GMT]]></content>
      <tags>
        <tag>kubernetes</tag>
        <tag>kubeadm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes环境中flannel网络插件的DNS与hosts文件的优先级问题]]></title>
    <url>%2F2019%2F07%2F27%2Fkubernetes%E7%8E%AF%E5%A2%83%E4%B8%ADflannel%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E7%9A%84DNS%E4%B8%8Ehosts%E6%96%87%E4%BB%B6%E7%9A%84%E4%BC%98%E5%85%88%E7%BA%A7%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[起因最近收到在中国XX银行部署kubernetes的同事的求救,他的kubernetes环境中使用了flannel网络插件,刚部署的时候一切正常一旦节点重启后网络插件就无法正常启动(状况如下图),为此他已经困扰了好几天了. 为了找出问题根因,同事在公司内部用虚拟机搭建了测试环境并且问题成功复现,剩下的就是我的找bug之路. 分析从上图可以看出flannel在访问apiserver(https://slb.mpaas.com)时超时.下面从这三个方面分析 是否能解析该url地址 网络是否互通 服务端口是否被阻塞 先看看我们测试环境的抛错 12345678910[root@deplaynode ~]# kubectl get pods -n kube-system -owide |grep flannelkube-flannel-ds-4gtr6 1/1 Running 0 1d 192.168.65.129 192.168.65.129kube-flannel-ds-4t8mg 1/1 Running 1 1d 192.168.65.127 192.168.65.127kube-flannel-ds-bp8fp 1/1 Running 0 18m 192.168.65.158 192.168.65.158kube-flannel-ds-dxm5z 0/1 CrashLoopBackOff 3 3m 192.168.65.128 192.168.65.128[root@deplaynode ~]# kubectl logs -n kube-system kube-flannel-ds-dxm5z I0727 11:01:28.083926 1 main.go:488] Using interface with name eth0 and address 192.168.65.128I0727 11:01:28.084163 1 main.go:505] Defaulting external address to interface address (192.168.65.128)E0727 11:01:58.182010 1 main.go:232] Failed to create SubnetManager: error retrieving pod spec for 'kube-system/kube-flannel-ds-dxm5z': Get https://slb.czl.com:6443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-dxm5z: dial tcp: i/o timeout[root@deplaynode ~]# ​ 下面我们进入这个节点手动把这个节点的falannel起来并进入测试 1234567891011121314[root@deplaynode ~]# ssh 192.168.65.128Last login: Sat Jul 27 07:05:27 2019 from deplaynode[root@node3 ~]# docker ps -a|grep kube-flannel-ds-dxm5zb007d4443f57 f0fad859c909 "/opt/bin/flanneld -…" 2 minutes ago Exited (1) 28 seconds ago k8s_kube-flannel_kube-flannel-ds-dxm5z_kube-system_89784a73-b05d-11e9-ac40-52545f91669f_5bf5e552ca81b f0fad859c909 "cp -f /etc/kube-fla…" 7 minutes ago Exited (0) 7 minutes ago k8s_install-cni_kube-flannel-ds-dxm5z_kube-system_89784a73-b05d-11e9-ac40-52545f91669f_0d417141adb57 mirrorgooglecontainers/pause-amd64:3.1 "/pause" 7 minutes ago Up 7 minutes k8s_POD_kube-flannel-ds-dxm5z_kube-system_89784a73-b05d-11e9-ac40-52545f91669f_0[root@node3 ~]# docker start b007d4443f57b007d4443f57[root@node3 ~]# docker exec -it b007d4443f57 sh/ # ping slb.czl.comPING slb.czl.com (192.168.65.158): 56 data bytes64 bytes from 192.168.65.158: seq=0 ttl=64 time=0.567 ms64 bytes from 192.168.65.158: seq=1 ttl=64 time=0.526 ms64 bytes from 192.168.65.158: seq=2 ttl=64 time=0.635 ms 从上图中我们可以看到,我们手动把flannel容器启动,并进入容器ping我们的域名时是通的,此可以证明容器时可以解析域名的,并且链路上是通的,但不幸很短时间内容器就退出了我们没有办法进行进一步测试. 我们尝试手动为这个POD再注入1个centos容器并进行进一步测试. 123456789101112131415161718192021222324252627282930313233343536373839[root@node3 ~]# docker run --rm -it --net=container:d417141adb57 centos:7 shsh-4.2# telnetsh: telnet: command not foundsh-4.2# curl -kv https://slb.czl.com:6443 * About to connect() to slb.czl.com port 6443 (#0)* Trying 192.168.65.158...* Connected to slb.czl.com (192.168.65.158) port 6443 (#0)* Initializing NSS with certpath: sql:/etc/pki/nssdb* skipping SSL peer certificate verification* NSS: client certificate not found (nickname not specified)* SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256* Server certificate:* subject: CN=kubernetes,OU=System,O=k8s,L=XS,ST=HangZhou,C=CN* start date: Jul 11 12:27:00 2019 GMT* expire date: Jul 08 12:27:00 2029 GMT* common name: kubernetes* issuer: CN=kubernetes,OU=System,O=k8s,L=XS,ST=HangZhou,C=CN&gt; GET / HTTP/1.1&gt; User-Agent: curl/7.29.0&gt; Host: slb.czl.com:6443&gt; Accept: */*&gt; &lt; HTTP/1.1 401 Unauthorized&lt; Content-Type: application/json&lt; Www-Authenticate: Basic realm="kubernetes-master"&lt; Date: Sat, 27 Jul 2019 11:20:22 GMT&lt; Content-Length: 165&lt; &#123; "kind": "Status", "apiVersion": "v1", "metadata": &#123; &#125;, "status": "Failure", "message": "Unauthorized", "reason": "Unauthorized", "code": 401* Connection #0 to host slb.czl.com left intact 我们发现该POD与 https://slb.czl.com:6443 是可以通行的,但为什么falannel还一直报timeout错误呢?我们继续向下查. 123456789101112131415[root@node3 ~]# docker run --rm -it --net=container:d417141adb57 centos:7 shsh-4.2# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.65.126 deplaynode192.168.65.158 node1192.168.65.127 node2192.168.65.128 node3192.168.65.129 node4192.168.65.158 slb.czl.comsh-4.2# cat /etc/resolv.conf nameserver 10.68.0.2search kube-system.svc.cluster.local. svc.cluster.local. cluster.local.options ndots:5sh-4.2# 我们可以看到该POD是通过/etc/hosts文件解析slb.czl.com的.并且使用了集群的DNS服务器. 123456[root@deplaynode ~]# kubectl get pods -n kube-system -owide |grep flannelkube-flannel-ds-4gtr6 1/1 Running 0 1d 192.168.65.129 192.168.65.129kube-flannel-ds-4t8mg 1/1 R unning 1 1d 192.168.65.127 192.168.65.127kube-flannel-ds-bp8fp 1/1 Running 0 47m 192.168.65.158 192.168.65.158kube-flannel-ds-dxm5z 0/1 CrashLoopBackOff 9 32m 192.168.65.128 192.168.65.128[root@deplaynode ~]# kubectl get pods -n kube-system kube-flannel-ds-dxm5z -oyaml 我们发现slb.czl.com是通过环境变量注入,我们直接改成IP地址试试 1kubectl edit ds -n kube-system kube-flannel-ds 1kubectl delete pods -n kube-system kube-flannel-ds-dxm5z 我们发现POD已经正常了,为什么使用IP地址可以,使用域名就不行呢?肯定是域名解析用问题. 我们继续查 1kubectl get ds -n kube-system kube-flannel-ds -oyaml 我们可以看见POD的spec中定义了POD使用集群的DNS和本地的hosts文件,如果我们不填集群的DNS呢? 我们把dnsPolicy改成ClusterFirst或Default (这两者在hostNetwork下等价参见) 1kubectl edit ds -n kube-system kube-flannel-ds 1kubectl delete pods -n kube-system kube-flannel-ds-2hnlj 我们发现同样flannel可以正常启动,我们进入容器看看 1kubectl exec -it -n kube-system kube-flannel-ds-5np99 sh 我们发现在没有设置DNS服务器的时候fannel可以正常工作,一旦设置DNS服务器就不行,难道是DNS与/etc/hosts的优先级发生了变化? 经过一番google得到两篇这样文章/etc/hosts文件在alpine镜像中对go程序无效 Notes on the NSS Configuration File 意思就是golang需要查看/etc/nsswitch.conf确定DNS服务器与hosts文件的优先级,如果没有/etc/nsswitch.conf则默认DNS服务器优先级高. 现在我们看看flannel中是否有这个文件 1234[root@deplaynode ~]# kubectl exec -it -n kube-system kube-flannel-ds-5np99 sh/ # ls /etc/nsswitch.confls: /etc/nsswitch.conf: No such file or directory/ # 现在我们手动构建镜像看看 12345678910111213[root@node3 flannel]# cat Dockerfile FROM jmgao1983/flannel:v0.10.0-amd64RUN echo "hosts: files dns" &gt; /etc/nsswitch.conf[root@node3 flannel]# docker build -t jmgao1983/flannel:v0.10.1-amd64 .Sending build context to Docker daemon 2.048kBStep 1/2 : FROM jmgao1983/flannel:v0.10.0-amd64 ---&gt; f0fad859c909Step 2/2 : RUN echo "hosts: files dns" &gt; /etc/nsswitch.conf ---&gt; Running in 25b873512a11Removing intermediate container 25b873512a11 ---&gt; d031bc89c05dSuccessfully built d031bc89c05dSuccessfully tagged jmgao1983/flannel:v0.10.1-amd64 1kubectl edit ds -n kube-system kube-flannel-ds 1kubectl delete pod -n kube-system kube-flannel-ds-5np99 我们看到flannel也正常启动了. 那么问题来了:为什么刚安装集群的时候没有报错? 看同事的安装脚本发现网络插件flannel优先于DNS插件安装,也就是在刚开始安装flannel的时候还没用DNS的service,endpiont和POD所以flannel直接找/etc/hosts了. 结论 flannel POD的DNS策略为ClusterFirstWithHostNet使得flanne可以同时使用集群DNS和本地hostswenjian flannel由yougo语言编写,go语言的网络库依赖/etc/nsswitch.conf确定DNS服务器与/etc/hosts的优先级 flannel使用了apline的基础镜像,其中不含/etc/nsswitch.conf文件 flannel解析DNS地址一直不返回造成超时 解决方案自定义镜像添加/etc/nsswitch.conf文件调整DNS服务器与/etc/hosts的优先级,dockerfile如下 12FROM jmgao1983/flannel:v0.10.0-amd64RUN echo "hosts: files dns" &gt; /etc/nsswitch.conf]]></content>
      <tags>
        <tag>kubernetes</tag>
        <tag>flannel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用operator-sdk构建kubernetesCRD控制器实现自动申请TLS证书]]></title>
    <url>%2F2019%2F07%2F12%2F%E4%BD%BF%E7%94%A8operator-sdk%E6%9E%84%E5%BB%BAkubernetesCRD%E6%8E%A7%E5%88%B6%E5%99%A8%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E7%94%B3%E8%AF%B7TLS%E8%AF%81%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[cert-operator项目由来早些时候写过一个kubernetes自定义资源CRD的controller,主要是声明一个证书需求,由控制器自动申请一个证书,并保存为一个secret,项目路径cert-controller 本项目与上述项目功能一致,只不过是用了一个框架对其做了重构. 项目 cert-controller cert-operator 框架 sample-apiserver operator-sdk 框架维护者 kubernetes官方社区 coreOS 证书申请方式 acme acme 证书申请软件 acme.sh lego 证书软件调用方式 容器中安装acme.sh软件+go调用系统命令 go调用lego库 operator-sdk成熟度较高,且在openshift中有大量应用.本项目摒弃了上个项目调用系统命令的低效调用方式,直接采用golang调用lego(也为golang开发)原生库. 项目构建1234mkdir -p $GOPATH/src/github.com/fanfengqiangcd $GOPATH/src/github.com/fanfengqianggit clone git@github.com:fanfengqiang/cert-operator.gitoperator-sdk build fanfengqiang/cert-operator:v1.0 使用说明1.创建自定义资源CRD 123456789# 创建CRD资源kubectl apply -f deploy/crds/certoperator_v1beta1_cert_crd.yaml# 创建控制器所需的SAkubectl apply -f deploy/service_account.yaml# 创建角色和角色绑定kubectl apply -f deploy/clusterrole.yamlkubectl apply -f deploy/clusterrole_binding.yaml# 创建控制器kubectl apply -f deploy/operator.yaml 2.编写cert资源清单并应用 1234567891011121314# cat deploy/crds/certoperator_v1beta1_cert_cr.yamlapiVersion: certoperator.5ik8s.com/v1beta1kind: Certmetadata: name: www.5ik8s.comspec: secretName: www.5ik8s.com domain: www.5ik8s.com email: "1415228958@qq.com" validityPeriod: 30 provider: alidns envs: ALICLOUD_ACCESS_KEY: "LTAIXXXXXXXXXyzLu" ALICLOUD_SECRET_KEY: "iH5sCTf4CzXXXXXXX9GLL2AsLW2" 12# 应用资源清单kubectl apply -f deploy/crds/certoperator_v1beta1_cert_cr.yaml 3.参数定义 参数 含义 .metadata.name cert资源的名字 .spec.secretName 生成的secret的名字 .spec.domain 生成证书的域名 .spec.validityPeriod secret的有效时长，单位天，范围1~89 .spec.provider 域名托管商的标示 .spec.envs 域名托管商API的accesskey和secret 完整域名托管商的格式，accesskey和secret格式参见 致谢本项目参考了如下两个项目 memcached-operator lego]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二进制部署k8s-HA集群.md]]></title>
    <url>%2F2019%2F07%2F10%2F%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2k8s-HA%E9%9B%86%E7%BE%A4-md%2F</url>
    <content type="text"><![CDATA[1. 配置基础环境1.0. 配置要求centos7操作系统且已安装epel源 主机 角色 组件 IP VIP Node1 Master,node etcd,api-server,controller-manager,scheduler,kubelet,kube-proxy,haproxy,keepalive 192.168.66.41 192.168.66.40 Node2 Master,node etcd,api-server,controller-manager,scheduler,kubelet,kube-proxy,haproxy,keepalive 192.168.66.42 192.168.66.40 Node3 Master,node etcd,api-server,controller-manager,scheduler,kubelet,kube-proxy,haproxy,keepalive 192.168.66.43 192.168.66.40 Node4 Node kubelet,kube-proxy 192.168.66.44 service网络:10.96.0.0/16 pod网络:10.244.0.0/16 1.1. 关闭防火墙12systemctl disable firewalld.service systemctl stop firewalld.service 1.2. 关闭selinux12setenforce 0sed -i.bak 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 1.3. 关闭swap12sed -ri '/^[^#]/s@(.*swap.*)@# \1@' /etc/fstabswapoff -a 1.4. 配置时间同步12345# 同步时间yum install -y ntpdate chronyntpdate -u ntp.api.bzsystemctl enable chronyd.servicesystemctl restart chronyd.service 1.5. 升级内核123456# 升级内核rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm yum --enablerepo=elrepo-kernel install kernel-lt-devel kernel-lt -ygrub2-set-default 0reboot 1.6. 安装docker123456789101112131415161718192021222324# step 1: 安装必要的一些系统工具sudo yum install -y yum-utils device-mapper-persistent-data lvm2# Step 2: 添加软件源信息sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# Step 3: 更新并安装 Docker-CEsudo yum makecache fastsudo yum -y install docker-ce-18.06.3.ce-3.el7# 注意：# 官方软件源默认启用了最新的软件，您可以通过编辑软件源的方式获取各个版本的软件包。例如官方并没有将测试版本的软件源置为可用，你可以通过以下方式开启。同理可以开启各种测试版本等。# vim /etc/yum.repos.d/docker-ce.repo# 将 [docker-ce-test] 下方的 enabled=0 修改为 enabled=1## 安装指定版本的Docker-CE:# Step 1: 查找Docker-CE的版本:# yum list docker-ce.x86_64 --showduplicates | sort -r# Loading mirror speeds from cached hostfile# Loaded plugins: branch, fastestmirror, langpacks# docker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stable# docker-ce.x86_64 17.03.1.ce-1.el7.centos @docker-ce-stable# docker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stable# Available Packages# Step2 : 安装指定版本的Docker-CE: (VERSION 例如上面的 17.03.0.ce.1-1.el7.centos)# sudo yum -y install docker-ce-[VERSION] 1.7. 取消docker启动后iptables转发限制1234567891011121314151617181920212223mkdir -p /lib/systemd/system/docker.service.dcat &gt;/lib/systemd/system/docker.service.d/iptables.conf &lt;&lt; EOF[Service]ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPTEOFmkdir -p /etc/docker/cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; "exec-opts": ["native.cgroupdriver=systemd"], "log-driver": "json-file", "log-opts": &#123; "max-size": "100m" &#125;, "storage-driver": "overlay2", "storage-opts": [ "overlay2.override_kernel_check=true" ]&#125;EOFsystemctl daemon-reloadsystemctl enable docker.servicesystemctl restart docker.service 1.8. 调整内核参数123456789#写入配置文件cat &lt;&lt;EOF &gt; /etc/sysctl.d/docker.confvm.swappiness = 0net.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF#生效配置文件sysctl -p /etc/sysctl.d/docker.conf 1.9. 加载ipvs模块12345678910111213cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashipvs_modules="ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4"for kernel_module in \$&#123;ipvs_modules&#125;; do /sbin/modinfo -F filename \$&#123;kernel_module&#125; &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe \$&#123;kernel_module&#125; fidoneEOFchmod 755 /etc/sysconfig/modules/ipvs.modulesbash /etc/sysconfig/modules/ipvs.moduleslsmod | grep ip_vs 2. 配置证书2.1. 下载证书生成工具cfssl1234wget -O /bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget -O /bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget -O /bin/cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64for cfssl in `ls /bin/cfssl*`;do chmod +x $cfssl;done; 2.2. 生成etcd集群证书2.2.0. 设置k8s环境12345export MASTER_VIP=192.168.66.40export MASTER_01_IP=192.168.66.41export MASTER_02_IP=192.168.66.42export MASTER_03_IP=192.168.66.43export K8S_SVC=10.96.0.1 2.2.1. 生成etcd集群CA1234567891011121314151617181920212223242526272829303132333435363738394041424344mkdir -p ~/k8s/pki/etcd/cd ~/k8s/pki/etcd/cat &lt;&lt; EOF | tee ca-config.json&#123; "signing": &#123; "default": &#123; "expiry": "87600h" &#125;, "profiles": &#123; "etcd": &#123; "expiry": "87600h", "usages": [ "signing", "key encipherment", "server auth", "client auth" ] &#125; &#125; &#125;&#125;EOF# 注：# ① signing ：表示该证书可用于签名其它证书，生成的 ca.pem 证书中CA=TRUE ；# ② server auth ：表示 client 可以用该该证书对 server 提供的证书进行验证；# ③ client auth ：表示 server 可以用该该证书对 client 提供的证书进行验证；cat &lt;&lt; EOF | tee ca-csr.json&#123; "CN": "etcd CA", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "L": "Beijing", "ST": "Beijing" &#125; ]&#125;EOFcfssl gencert -initca ca-csr.json | cfssljson -bare ca 2.2.2. 生成etcd服务端证书1234567891011121314151617181920212223cat &lt;&lt; EOF | tee server-csr.json&#123; "CN": "etcd", "hosts": [ "$&#123;MASTER_01_IP&#125;", "$&#123;MASTER_02_IP&#125;", "$&#123;MASTER_03_IP&#125;", "127.0.0.1" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "L": "Beijing", "ST": "Beijing" &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd server-csr.json | cfssljson -bare server 2.2.3. 生成etcd心跳客户端证书1234567891011121314151617181920212223cat &lt;&lt; EOF | tee peer-csr.json&#123; "CN": "peer", "hosts": [ "$&#123;MASTER_01_IP&#125;", "$&#123;MASTER_02_IP&#125;", "$&#123;MASTER_03_IP&#125;", "127.0.0.1" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "L": "Beijing", "ST": "Beijing" &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd peer-csr.json | cfssljson -bare peer 2.2.4. 生成k8s客户端证书12345678910111213141516171819202122232425cat &lt;&lt; EOF | tee kubernetes-etcd-csr.json&#123; "CN": "kube-apiserver-etcd-client", "hosts": [ "$&#123;MASTER_01_IP&#125;", "$&#123;MASTER_02_IP&#125;", "$&#123;MASTER_03_IP&#125;", "$&#123;MASTER_VIP&#125;", "127.0.0.1" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "L": "Beijing", "O": "system:masters", "ST": "Beijing" &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd kubernetes-etcd-csr.json | cfssljson -bare kubernetes-etcd 2.3. 生成K8S集群证书2.3.1. 生成K8S集群CA123456789101112131415161718192021222324252627282930313233343536373839404142mkdir -p ~/k8s/pki/k8scd ~/k8s/pki/k8scat &lt;&lt; EOF |tee ca-config.json&#123; "signing": &#123; "default": &#123; "expiry": "87600h" &#125;, "profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "87600h" &#125; &#125; &#125;&#125;EOFcat &lt;&lt; EOF |tee ca-csr.json&#123; "CN": "kubernetes", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOFcfssl gencert -initca ca-csr.json | cfssljson -bare ca 2.3.2. 生成kube-apiserver证书1234567891011121314151617181920212223242526272829303132cat &lt;&lt; EOF|tee kubernetes-csr.json&#123; "CN": "kubernetes", "hosts": [ "127.0.0.1", "$&#123;MASTER_01_IP&#125;", "$&#123;MASTER_02_IP&#125;", "$&#123;MASTER_03_IP&#125;", "$&#123;MASTER_VIP&#125;", "$&#123;K8S_SVC&#125;", "kubernetes", "kubernetes.default", "kubernetes.default.svc", "kubernetes.default.svc.cluster", "kubernetes.default.svc.cluster.local" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes 2.3.3. 生成kube-controllor证书1234567891011121314151617181920212223242526cat &gt; kube-controller-manager-csr.json &lt;&lt; EOF&#123; "CN": "system:kube-controller-manager", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "hosts": [ "127.0.0.1", "$&#123;MASTER_01_IP&#125;", "$&#123;MASTER_02_IP&#125;", "$&#123;MASTER_03_IP&#125;", "$&#123;MASTER_VIP&#125;" ], "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "system:kube-controller-manager", "OU": "System" &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager 2.3.4. 生成kube-scheduler证书1234567891011121314151617181920212223242526cat &gt; kube-scheduler-csr.json &lt;&lt; EOF&#123; "CN": "system:kube-scheduler", "hosts": [ "127.0.0.1", "$&#123;MASTER_01_IP&#125;", "$&#123;MASTER_02_IP&#125;", "$&#123;MASTER_03_IP&#125;", "$&#123;MASTER_VIP&#125;" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "system:kube-scheduler", "OU": "System" &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler 2.3.5. 生成kubectl证书1234567891011121314151617181920cat &lt;&lt; EOF|tee admin-csr.json&#123; "CN": "admin", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "system:masters", "OU": "System" &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin 2.4. 分发证书12345for host in $MASTER_01_IP $MASTER_02_IP $MASTER_03_IP;do ssh root@$&#123;host&#125; "mkdir -p /etc/etcd/ssl;mkdir -p /etc/kubernetes/ssl" scp ~/k8s/pki/etcd/* root@$&#123;host&#125;:/etc/etcd/ssl/ scp ~/k8s/pki/k8s/* root@$&#123;host&#125;:/etc/kubernetes/ssldone 3. 分发二进制文件3.1. etcd123456789cd ~wget https://github.com/etcd-io/etcd/releases/download/v3.3.13/etcd-v3.3.13-linux-amd64.tar.gztar xvf etcd-v3.3.13-linux-amd64.tar.gzcd etcd-v3.3.13-linux-amd64for host in $MASTER_01_IP $MASTER_02_IP $MASTER_03_IP;do for cmd in "etcd etcdctl";do scp $&#123;cmd&#125; root@$&#123;host&#125;:/usr/bin/ donedone 3.2. k8s123456789cd ~wget https://storage.googleapis.com/kubernetes-release/release/v1.15.0/kubernetes-server-linux-amd64.tar.gztar xvf kubernetes-server-linux-amd64.tar.gzcd kubernetes/server/bin/for host in $MASTER_01_IP $MASTER_02_IP $MASTER_03_IP;do for cmd in "kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy kubectl kubeadm";do scp $&#123;cmd&#125; root@$&#123;host&#125;:/usr/bin/ donedone 4. 配置ETCD集群4.1. 创建 etcd 的 systemd unit 文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899cd ~mkdir -p k8s/etcdcd k8s/etcdcat &lt;&lt; EOF |tee etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.confExecStart=/usr/bin/etcd Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFindex=1for node in $MASTER_01_IP $MASTER_02_IP $MASTER_03_IP;docat &gt; $&#123;node&#125;_etcd.conf &lt;&lt; EOF #[Member]#ETCD_CORS=""ETCD_DATA_DIR="/var/lib/etcd"#ETCD_WAL_DIR=""ETCD_LISTEN_PEER_URLS="https://$&#123;node&#125;:2380"ETCD_LISTEN_CLIENT_URLS="https://localhost:2379,https://$&#123;node&#125;:2379"#ETCD_MAX_SNAPSHOTS="5"#ETCD_MAX_WALS="5"ETCD_NAME="node$&#123;index&#125;"ETCD_SNAPSHOT_COUNT="100000"#ETCD_HEARTBEAT_INTERVAL="100"#ETCD_ELECTION_TIMEOUT="1000"#ETCD_QUOTA_BACKEND_BYTES="0"#ETCD_MAX_REQUEST_BYTES="1572864"#ETCD_GRPC_KEEPALIVE_MIN_TIME="5s"#ETCD_GRPC_KEEPALIVE_INTERVAL="2h0m0s"#ETCD_GRPC_KEEPALIVE_TIMEOUT="20s"##[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS="https://$&#123;node&#125;:2380"ETCD_ADVERTISE_CLIENT_URLS="https://$&#123;node&#125;:2379"#ETCD_DISCOVERY=""#ETCD_DISCOVERY_FALLBACK="proxy"#ETCD_DISCOVERY_PROXY=""#ETCD_DISCOVERY_SRV=""ETCD_INITIAL_CLUSTER="node1=https://$&#123;MASTER_01_IP&#125;:2380,node2=https://$&#123;MASTER_02_IP&#125;:2380,node3=https://$&#123;MASTER_03_IP&#125;:2380"ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"ETCD_INITIAL_CLUSTER_STATE="new"#ETCD_STRICT_RECONFIG_CHECK="true"#ETCD_ENABLE_V2="true"##[Proxy]#ETCD_PROXY="off"#ETCD_PROXY_FAILURE_WAIT="5000"#ETCD_PROXY_REFRESH_INTERVAL="30000"#ETCD_PROXY_DIAL_TIMEOUT="1000"#ETCD_PROXY_WRITE_TIMEOUT="5000"#ETCD_PROXY_READ_TIMEOUT="0"##[Security]ETCD_CERT_FILE="/etc/etcd/ssl/server.pem"ETCD_KEY_FILE="/etc/etcd/ssl/server-key.pem"ETCD_CLIENT_CERT_AUTH="true"ETCD_TRUSTED_CA_FILE="/etc/etcd/ssl/ca.pem"#ETCD_AUTO_TLS="false"ETCD_PEER_CERT_FILE="/etc/etcd/ssl/peer.pem"ETCD_PEER_KEY_FILE="/etc/etcd/ssl/peer-key.pem"ETCD_PEER_CLIENT_CERT_AUTH="true"ETCD_PEER_TRUSTED_CA_FILE="/etc/etcd/ssl/ca.pem"#ETCD_PEER_AUTO_TLS="false"##[Logging]#ETCD_DEBUG="false"#ETCD_LOG_PACKAGE_LEVELS=""#ETCD_LOG_OUTPUT="default"##[Unsafe]#ETCD_FORCE_NEW_CLUSTER="false"##[Version]#ETCD_VERSION="false"#ETCD_AUTO_COMPACTION_RETENTION="0"##[Profiling]#ETCD_ENABLE_PPROF="false"#ETCD_METRICS="basic"##[Auth]#ETCD_AUTH_TOKEN="simple"EOFlet index++done 4.2. 分发配置123456index=1for node in $MASTER_01_IP $MASTER_02_IP $MASTER_03_IP;do scp $&#123;node&#125;_etcd.conf root@$&#123;node&#125;:/etc/etcd/ scp etcd.service root@$&#123;node&#125;:/lib/systemd/system/ ssh root@$&#123;node&#125; "mkdir -p /var/lib/etcd;systemctl daemon-reload"done 4.3. 启动 etcd 服务1234# 在每个节点上分别执行systemctl enable etcd# 启动第一个节点时不会直接返回,需要至少两个节点都启动后才返回systemctl start etcd 4.4. 验证服务12export ETCDCTL_API=3etcdctl --cacert /etc/etcd/ssl/ca.pem --cert /etc/etcd/ssl/kubernetes-etcd.pem --key /etc/etcd/ssl/kubernetes-etcd-key.pem member list 5. 创建 kubeconfig 文件创建 TLS Bootstrapping Token123456mkdir -p ~/k8s/kubeconfig/cd ~/k8s/kubeconfig/export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')cat &gt; token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,"system:kubelet-bootstrap"EOF ###创建 kubectl kubeconfig 文件 123456789101112131415161718192021kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://$&#123;MASTER_VIP&#125;:6443 \ --kubeconfig=admin.kubeconfig# 设置客户端认证参数kubectl config set-credentials admin \ --client-certificate=/etc/kubernetes/ssl/admin.pem \ --client-key=/etc/kubernetes/ssl/admin-key.pem \ --embed-certs=true \ --kubeconfig=admin.kubeconfig# 设置上下文参数kubectl config set-context kubernetes \ --cluster=kubernetes \ --user=admin \ --kubeconfig=admin.kubeconfig # 设置默认上下文kubectl config use-context kubernetes --kubeconfig=admin.kubeconfig 创建 controller-manager kubeconfig 文件123456789101112131415161718192021kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://192.168.66.41:6443 \ --kubeconfig=controller-manager.kubeconfig# 设置客户端认证参数kubectl config set-credentials system:kube-controller-manager \ --client-certificate=/etc/kubernetes/ssl/kube-controller-manager.pem \ --client-key=/etc/kubernetes/ssl/kube-controller-manager-key.pem \ --embed-certs=true \ --kubeconfig=controller-manager.kubeconfig# 设置上下文参数kubectl config set-context system:kube-controller-manager@kubernetes \ --cluster=kubernetes \ --user=system:kube-controller-manager \ --kubeconfig=controller-manager.kubeconfig # 设置默认上下文kubectl config use-context system:kube-controller-manager@kubernetes --kubeconfig=controller-manager.kubeconfig 创建 scheduler kubeconfig 文件123456789101112131415161718192021kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://192.168.66.41:6443 \ --kubeconfig=scheduler.kubeconfig# 设置客户端认证参数kubectl config set-credentials system:kube-scheduler \ --client-certificate=/etc/kubernetes/ssl/kube-scheduler.pem \ --client-key=/etc/kubernetes/ssl/kube-scheduler-key.pem \ --embed-certs=true \ --kubeconfig=scheduler.kubeconfig# 设置上下文参数kubectl config set-context system:kube-scheduler@kubernetes \ --cluster=kubernetes \ --user=system:kube-scheduler \ --kubeconfig=scheduler.kubeconfig # 设置默认上下文kubectl config use-context system:kube-scheduler@kubernetes --kubeconfig=scheduler.kubeconfig 分发123for node in $MASTER_01_IP $MASTER_02_IP $MASTER_03_IP;do scp * root@$&#123;node&#125;:/etc/kubernetes/done 6. 部署master节点配置负载均衡keepalive+lvs123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687mkdir -p ~/k8s/lvscd ~/k8s/lvsPRIORITY=100STATE="MASTER"NET_IF="eth0"index=1for node in $MASTER_01_IP $MASTER_02_IP $MASTER_03_IP ;do ssh root@$&#123;node&#125; "yum install -y keepalived ipvsadm" echo """global_defs &#123; router_id LVS_DEVEL&#125;vrrp_instance VI_1 &#123; state $&#123;STATE&#125; interface $&#123;NET_IF&#125; virtual_router_id 80 priority $&#123;PRIORITY&#125; advert_int 1 authentication &#123; auth_type PASS auth_pass just0kk &#125; virtual_ipaddress &#123; $&#123;MASTER_VIP&#125; &#125;&#125;virtual_server $&#123;MASTER_VIP&#125; 6443 &#123; delay_loop 6 lb_algo rr lb_kind DR nat_mask 255.255.255.0 persistence_timeout 0 protocol TCP real_server $MASTER_01_IP 6443 &#123; weight 1 SSL_GET &#123; url &#123; path /healthz status_code 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125; real_server $MASTER_02_IP 6443 &#123; weight 1 SSL_GET &#123; url &#123; path /healthz status_code 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125; real_server $MASTER_03_IP 6443 &#123; weight 1 SSL_GET &#123; url &#123; path /healthz status_code 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125;&#125;""" &gt; ./keepalived-$&#123;index&#125;.conf scp ./keepalived-$&#123;index&#125;.conf root@$&#123;node&#125;:/etc/keepalived/keepalived.conf ssh root@$&#123;node&#125; """ systemctl stop keepalived systemctl enable keepalived systemctl start keepalived """ let index++ STATE="BACKUP" let PRIORITY=PRIORITY-20done kube-apiserver123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051mkdir -p ~/k8s/service/apiservercd ~/k8s/service/apiserverindex=1for node in $MASTER_01_IP $MASTER_02_IP $MASTER_03_IP;docat &gt; $&#123;node&#125;_kube-apiserver.service &lt;&lt; EOF[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]ExecStart=/usr/bin/kube-apiserver \ --enable-admission-plugins=NodeRestriction \ --advertise-address=$&#123;MASTER_VIP&#125; \ --bind-address=$&#123;node&#125; \ --insecure-port=0 \ --authorization-mode=Node,RBAC \ --enable-bootstrap-token-auth \ --token-auth-file=/etc/kubernetes/token.csv \ --service-cluster-ip-range=10.96.0.0/12 \ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \ --client-ca-file=/etc/kubernetes/ssl/ca.pem \ --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \ --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \ --etcd-cafile=/etc/etcd/ssl/ca.pem \ --etcd-certfile=/etc/etcd/ssl/kubernetes-etcd.pem \ --etcd-keyfile=/etc/etcd/ssl/kubernetes-etcd-key.pem \ --etcd-servers=https://$&#123;MASTER_01_IP&#125;:2379,https://$&#123;MASTER_02_IP&#125;:2379,https://$&#123;MASTER_03_IP&#125;:2379 \ --allow-privileged=true \ --apiserver-count=3 Restart=on-failureRestartSec=5Type=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.targeEOFlet index++donefor node in $MASTER_01_IP $MASTER_02_IP $MASTER_03_IP;do scp $&#123;node&#125;_kube-apiserver.service root@$&#123;node&#125;:/lib/systemd/system/kube-apiserver.service ssh root@$&#123;node&#125; """systemctl daemon-reload systemctl enable kube-apiserver.service systemctl restart kube-apiserver.service """done kube-controller-manager12345678910111213141516171819202122232425262728293031323334353637383940414243mkdir -p ~/k8s/service/controller-managercd ~/k8s/service/controller-managercat &gt; kube-controller-manager.service &lt;&lt; EOF[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]ExecStart=/usr/bin/kube-controller-manager \ --bind-address=127.0.0.1 \ --allocate-node-cidrs=true \ --cluster-cidr=10.244.0.0/16 \ --node-cidr-mask-size=24 \ --kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \ --authentication-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \ --authorization-kubeconfig=/etc/kubernetes/controller-manager.kubeconfig \ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \ --service-cluster-ip-range=10.96.0.0/12 \ --root-ca-file=/etc/kubernetes/ssl/ca.pem \ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \ --tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \ --tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \ --leader-elect=true \ --controllers=*,bootstrapsigner,tokencleaner \ --use-service-account-credentials=true Restart=onRestart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOFfor node in $MASTER_01_IP $MASTER_02_IP $MASTER_03_IP;do scp kube-controller-manager.service root@$&#123;node&#125;:/lib/systemd/system/kube-controller-manager.service ssh root@$&#123;node&#125; """systemctl daemon-reload systemctl enable kube-controller-manager.service systemctl restart kube-controller-manager.service """done kube-scheduler1234567891011121314151617181920212223242526mkdir -p ~/k8s/service/schedulercd ~/k8s/service/schedulercat &gt; kube-scheduler.service &lt;&lt; EOF[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]ExecStart=/usr/bin/kube-scheduler \ --bind-address=127.0.0.1 \ --kubeconfig=/etc/kubernetes/scheduler.kubeconfig \ --leader-elect=true Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOFfor node in $MASTER_01_IP $MASTER_02_IP $MASTER_03_IP;do scp kube-scheduler.service root@$&#123;node&#125;:/lib/systemd/system/kube-scheduler.service ssh root@$&#123;node&#125; """systemctl daemon-reload systemctl enable kube-scheduler.service systemctl restart kube-scheduler.service """done 配置RBAC1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253mkdir -p ~/k8s/rbac/cd ~/k8s/rbac/# system:bootstrappers组用户有权限申请证书cat &gt; create-csrs-for-bootstrapping.yaml &lt;&lt; EOF# enable bootstrapping nodes to create CSRapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: create-csrs-for-bootstrappingsubjects:- kind: Group name: system:bootstrappers apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: system:node-bootstrapper apiGroup: rbac.authorization.k8s.ioEOF# system:bootstrappers组用户自动同意证书申请cat &gt; auto-approve-csrs-for-group.yaml &lt;&lt; EOF# Approve all CSRs for the group "system:bootstrappers"apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: auto-approve-csrs-for-groupsubjects:- kind: Group name: system:bootstrappers apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:nodeclient apiGroup: rbac.authorization.k8s.ioEOF# system:nodes组用户有权限自动更新证书cat &gt; auto-approve-renewals-for-nodes.yaml &lt;&lt; EOF# Approve renewal CSRs for the group "system:nodes"apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: auto-approve-renewals-for-nodessubjects:- kind: Group name: system:nodes apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient apiGroup: rbac.authorization.k8s.ioEOFkubectl apply . 7. 部署node节点创建 Bootstrapping kubeconfig 文件创建 TLS Bootstrapping Token可使用创建集群时的TOKEN文件“/etc/kubernetes/token.csv”;或者使用kubeadm新创建 123456export BOOTSTRAP_TOKEN=$(kubeadm token create \ --description kubelet-bootstrap-token \ --groups system:bootstrappers:k8s-master1 \ --kubeconfig ~/.kube/config)echo $BOOTSTRAP_TOKENkubeadm token list 创建 kubelet bootstrapping kubeconfig 文件123456789101112131415mkdir -p ~/k8s/nodecd ~/k8s/nodekubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://$&#123;MASTER_VIP&#125;:6443 \ --kubeconfig=bootstrap.kubeconfigkubectl config set-credentials kubelet-bootstrap \ --token=$&#123;BOOTSTRAP_TOKEN&#125; \ --kubeconfig=bootstrap.kubeconfigkubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=bootstrap.kubeconfigkubectl config use-context default --kubeconfig=bootstrap.kubeconfig 创建 kube-proxy bootstrapping kubeconfig 文件123456789101112131415kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=https://$&#123;MASTER_VIP&#125;:6443 \ --kubeconfig=kube-proxy.kubeconfigkubectl config set-credentials kube-proxy \ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.kubeconfigkubectl config set-context default \ --cluster=kubernetes \ --user=kube-proxy \ --kubeconfig=kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 配置kubelet12345678910111213141516171819202122232425262728293031323334353637383940414243444546export NODE_IP=192.168.66.41cat &gt; kubelet.config.yaml &lt;&lt;EOFkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1authentication: x509: &#123;clientCAFile: /etc/kubernetes/ssl/ca.pem&#125; webhook: &#123;enabled: true, cacheTTL: 2m0s&#125; anonymous: &#123;enabled: false&#125;authorization: mode: Webhook webhook: &#123;cacheAuthorizedTTL: 5m0s, cacheUnauthorizedTTL: 30s&#125;address: $&#123;NODE_IP&#125;port: 10250readOnlyPort: 0cgroupDriver: cgroupfshairpinMode: promiscuous-bridgeserializeImagePulls: falsefeatureGates: &#123;RotateKubeletClientCertificate: true, RotateKubeletServerCertificate: true&#125;clusterDomain: cluster.local.clusterDNS: [10.96.0.10]EOFcat &gt; kubelet.service &lt;&lt; EOF[Unit]Description=Kubernetes KubeletDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletExecStart=/usr/bin/kubelet \ --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \ --config=/etc/kubernetes/kubelet.config.yaml \ --cert-dir=/etc/kubernetes/ssl \ --cgroup-driver=systemd \ --network-plugin=cni \ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOFmkdir -p /var/lib/kubelet 未完,待续…..]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在kubernetes中部署Jenkins CICD工具并设置动态创建Jenkins slave]]></title>
    <url>%2F2019%2F05%2F04%2F%E5%9C%A8kubernetes%E4%B8%AD%E9%83%A8%E7%BD%B2Jenkins-CICD%E5%B7%A5%E5%85%B7%E5%B9%B6%E8%AE%BE%E7%BD%AE%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BAJenkins-slave%2F</url>
    <content type="text"><![CDATA[1. 安装Jenkins1.1. 编写manifests配置清单1.1.1. 创建名称空间我们创建一个名字叫jenkins的名称空间，后面所有的资源都放在这个名称空间下，方便管理。 123456# jenkins-ns.yaml ---apiVersion: v1kind: Namespacemetadata: name: jenkins 1.1.2. 创建PV/PVC因为我们要把设置的配置、安装的插件持久化下来，顾创建持久化存储卷，因我集群中有storageClass，顾创建PVC会为我动态生成PV。 123456789101112131415# jenkins-pvc.yaml ---kind: PersistentVolumeClaimapiVersion: v1metadata: name: jenkins-pvc namespace: jenkinsspec: accessModes: - ReadWriteMany resources: requests: storage: 20Gi storageClassName: ceph volumeMode: Filesystem 1.1.3. 创建RBAC因为我们要用jenkins动态创建jenkins slave POD，顾我们要赋予jenkins相应的权限来访问api server。 12345678910111213141516171819202122232425262728293031323334# jenkins-rbac.yaml ---apiVersion: v1kind: ServiceAccountmetadata: name: jenkins # 在jenkins名称空间下创建名字叫jenkins的SA namespace: jenkins---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: jenkins # 创建名字叫jenkins的ClusterRolerules: - apiGroups: ["extensions", "apps"] resources: ["deployments"] verbs: ["create", "delete", "get", "list", "watch", "patch", "update"] - apiGroups: [""] resources: ["services","pods","pods/exec","pods/log","secrets"] verbs: ["*"]---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: jenkinsroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: jenkinssubjects: - kind: ServiceAccount name: jenkins namespace: jenkins 1.1.4. 创建deployment1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# jenkins-deploy.yaml---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: jenkins namespace: jenkinsspec: template: metadata: labels: app: jenkins spec: terminationGracePeriodSeconds: 10 serviceAccountName: jenkins containers: - name: jenkins image: jenkins/jenkins:lts imagePullPolicy: IfNotPresent ports: - containerPort: 8080 name: web protocol: TCP - containerPort: 50000 name: agent protocol: TCP resources: limits: cpu: 1000m memory: 1Gi# requests:# cpu: 500m# memory: 512Mi livenessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 # ~2 minutes readinessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 # ~2 minutes volumeMounts: - name: jenkinshome subPath: jenkins mountPath: /var/jenkins_home env: - name: LIMITS_MEMORY valueFrom: resourceFieldRef: resource: limits.memory divisor: 1Mi - name: JAVA_OPTS value: -Xmx$(LIMITS_MEMORY)m -XshowSettings:vm -Dhudson.slaves.NodeProvisioner.initialDelay=0 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 -Duser.timezone=Asia/Shanghai securityContext: fsGroup: 1000 volumes: - name: jenkinshome persistentVolumeClaim: claimName: jenkins-pvc 1.1.5. 创建service12345678910111213141516171819# jenkins-svc.yaml---apiVersion: v1kind: Servicemetadata: name: jenkins namespace: jenkins labels: app: jenkinsspec: selector: app: jenkins ports: - name: web port: 8080 targetPort: 8080 - name: agent port: 50000 targetPort: 50000 1.1.6. 创建ingress12345678910111213141516# jenkins-ingress.yaml---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: jenkins namespace: jenkinsspec: rules: - host: jenkins.5ik8s.com http: paths: - path: / backend: serviceName: jenkins servicePort: 8080 1.2. 应用配置123456kubectl apply -f jenkins-ns.yamlkubectl apply -f jenkins-rbac.yamlkubectl apply -f jenkins-pvc.yamlkubectl apply -f jenkins-deploy.yamlkubectl apply -f jenkins-svc.yamlkubectl apply -f jenkins-ingress.yaml 1.3. 初始化 1kubectl exec -it -n jenkins jenkins-5dfc7749fb-tc59p -- cat /var/jenkins_home/secrets/initialAdminPassword 1.3.1. 安装推荐插件 1.3.2. 设置管理员密码 2. 配置jenkins2.1. 安装必要插件 2.2. 配置插件2.2.1. 重启jenkins2.2.2. 配置kubernetes启动jenkins slave的Pod模板 本文中用到的Docker镜像地址：fanfengqiang/jenkins_slave 3. 测试3.1. 创建项目 3.2. 配置项目 3.3. 触发任务3.4. 查看结果 从上图可以看出当用任务需要在jenkins slave上执行时k8s会自动创建相应Pods]]></content>
      <tags>
        <tag>kubernetes</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分享一张linux监控分析工具汇总图]]></title>
    <url>%2F2019%2F05%2F01%2F%E5%88%86%E4%BA%AB%E4%B8%80%E5%BC%A0linux%E7%9B%91%E6%8E%A7%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E6%B1%87%E6%80%BB%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[图片来源：http://www.brendangregg.com/Perf/linux_perf_tools_full.png]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用traefik作为kubernetes ingress controller并开启letsencrypt证书认证]]></title>
    <url>%2F2019%2F04%2F27%2F%E4%BD%BF%E7%94%A8traefik%E4%BD%9C%E4%B8%BAkubernetes-ingress-controller%E5%B9%B6%E5%BC%80%E5%90%AFletsencrypt%E8%AF%81%E4%B9%A6%E8%AE%A4%E8%AF%81%2F</url>
    <content type="text"><![CDATA[1. 前言在kubernetes集群中暴露服务主要有如下三种方式：loadbalancer ，NodePort ，Ingress NodePort：kube-proxy监控service资源，将service对象转化为iptables规则，iptables规则过滤拦截并转发流量到对应的Pod。优点：1.配置简单系统默认支持；缺点：1.iptables规则转发效率低，2.只要安装了kube-proxy组件的节点都会占用一个端口 Loadbalancer：采用NodePort接入流量，云平台监控service资源，为Loadbalancer分配一个负载均衡器，后端NodePort暴露的端口自动加入到负载均衡器。优点：1.可以直接互联网接入，不需要自己设置负载均衡器；缺点：1.需要云平台支持 Ingress：ingress controller监控ingress资源，根据ingress资源中包含的service资源找到对应提供服务的Pod地址，将流量通过Pod网络由ingress controller Pod直接发往后端提供服务的Pod，将ingress controller Pod部署在集群的边缘（以共享宿主机网络名称空间方式部署为deamonSet）可以将流量直接引入ingress controller Pod。优点：1.不需要iptables转发效率高，2.可以做HTTPS会话卸载（对外提供HTTPS协议服务，对内代用HTTP协议链接）；缺点：1.需要自己安装ingress controller 2. taefik简介常见的ingress controller有如下三种：Nginx ，traefik ，envoy nginx： 关于nginx作为负载均衡器不需要我多说大家都懂，但nginx本身并不是为云原生而设计的，如不支持配置动态加载，从环境变量获取配置等，nginx作为ingress controller的经典项目：ingress-nginx traefik: Træfɪk 是一个为了让部署微服务更加便捷而诞生的现代HTTP反向代理、负载均衡工具。 它支持多种后台 (Docker, Swarm, Kubernetes, Marathon, Mesos, Consul, Etcd, Zookeeper, BoltDB, Rest API, file…) 来自动化、动态的应用它的配置文件设置。(上面这段话摘自官方，总之一句话：云原生设计，支持多种后端，功能强大) traefik官方文档 envoy: Envoy 是专为大型现代 SOA（面向服务架构）架构设计的 L7 代理和通信总线.(云原生设计，XDS协议服务发现，C++11编写，功能非常强大，结构复杂概念多)envoy的经典应用有istio服务网格 ，istio服务网格将在以后的文章介绍。 本文介绍基于traefik实现ingress controller主要考虑如下： 云原生设计 自带kubernetes支持 安装配置简单 自带支持Lets encrypt证书申请 3. 安装traefik3.1. 下载traefik yaml文件文档地址 12345678# 如果就开启了rbac访问控制需要应用此配置文件wget https://raw.githubusercontent.com/containous/traefik/v1.7/examples/k8s/traefik-rbac.yaml# 因为我们要将traefik部署为共享宿主机网络名称空间，顾要部署为daemonSetwget https://raw.githubusercontent.com/containous/traefik/v1.7/examples/k8s/traefik-ds.yaml# traefik dashboard的测试Ingress资源wget https://raw.githubusercontent.com/containous/traefik/v1.7/examples/k8s/ui.yaml 3.2. 需改daemonSet资源配置文件修改traefik-ds.yaml文件 12345678910111213141516171819202122232425262728293031323334353637383940414243..... spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 hostNetwork: true # 改为共享宿主机名称空间 nodeSelector: # 添加节点选择器，固定traefik部署位置（需要提前给节点打label） edge: "true" tolerations: # 设置节点污点容忍度，如果想将traefik部署在master需要添加此项 - effect: NoSchedule operator: Exists containers: - image: traefik name: traefik-ingress-lb ports: - name: https # 添加https的端口 containerPort: 443 hostPort: 443 - name: http containerPort: 80 hostPort: 80 - name: admin containerPort: 8080 hostPort: 8080 securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE args: - --api - --kubernetes - --logLevel=INFO - --insecureSkipVerify=true # 如果后端服务是https协议时不验证其证书 - --defaultEntryPoints=http,https # traefik同时开启HTTP和HTTPS服务 - --entrypoints=Name:https Address::443 TLS # HTTPS服务监听在443端口 - --entrypoints=Name:http Address::80 Redirect.EntryPoint:https # HTTP监听在80端口，并将流量重定向至https---..... 3.3. 配置traefik自动获取lets encrypt证书（可选）本文使用的域名托管在阿里云，其他域名提供商参考文档修改 123456789101112131415161718192021222324252627282930313233343536373839..... containers: - image: fanfengqiang/traefik:1.7.10 # 官方镜像缺少依赖，下文中有镜像的制作方法 name: traefik-ingress-lb env: - name: ALICLOUD_ACCESS_KEY # 添加环境变量ALICLOUD_ACCESS_KEY value: LTAIwMtdOu8BBENJ # 阿里云RAM账号的access_key - name: ALICLOUD_SECRET_KEY # 添加环境变量ALICLOUD_SECRET_KEY value: h7ukpd1CrM2mWPRT52F1BZdq0w89CA # 阿里云RAM账号的access_secret ports:........ args: - --api - --kubernetes - --logLevel=INFO - --insecureSkipVerify=true - --defaultEntryPoints=http,https - --entrypoints=Name:https Address::443 TLS - --entrypoints=Name:http Address::80 Redirect.EntryPoint:https - --acme # 开启证书验证 - --acme.acmeLogging=true # 打开日志，方便排错 - --acme.email=1415228958@qq.com # 邮箱 - --acme.entryPoint=https # 证书类型 - --acme.storage=/tmp/acme.json # 证书申请临时文件 - --acme.dnschallenge # 域名验证方式 - --acme.dnschallenge.provider=alidns # 域名提供商 - --acme.dnschallenge.delaybeforecheck=5 # 验证域名延时 - --acme.httpchallenge.entrypoint=http # 验证域名时使用的协议 - --acme.domains=*.fanfengqiang.com # 要申请证书的域名 - --acme.domains=*.5ik8s.com # 要申请证书的域名---..... 镜像制作方法参见traefik_zoneinfo 完整traefik-ds.yaml配置文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091---apiVersion: v1kind: ServiceAccountmetadata: name: traefik-ingress-controller namespace: kube-system---kind: DaemonSetapiVersion: extensions/v1beta1metadata: name: traefik-ingress-controller namespace: kube-system labels: k8s-app: traefik-ingress-lbspec: template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 hostNetwork: true # 改为共享宿主机名称空间 nodeSelector: # 添加节点选择器，固定traefik部署位置（需要提前给节点打label） edge: "true" tolerations: # 设置节点污点容忍度，如果想将traefik部署在master需要添加此项 - effect: NoSchedule operator: Exists containers: - image: fanfengqiang/traefik:1.7.10 # 官方镜像缺少依赖，下文中有镜像的制作方法 name: traefik-ingress-lb env: - name: ALICLOUD_ACCESS_KEY # 添加环境变量ALICLOUD_ACCESS_KEY value: LTAIwXXXXXXXBENJ # 阿里云RAM账号的access_key - name: ALICLOUD_SECRET_KEY # 添加环境变量ALICLOUD_SECRET_KEY value: h7ukpXXXXXXXXxxT52F1BZdq0w89CA # 阿里云RAM账号的access_secret ports: - name: https containerPort: 443 hostPort: 443 - name: http containerPort: 80 hostPort: 80 - name: admin containerPort: 8080 hostPort: 8080 securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE args: - --api - --kubernetes - --logLevel=INFO - --insecureSkipVerify=true # 如果后端服务是https协议时不验证其证书 - --defaultEntryPoints=http,https # traefik同时开启HTTP和HTTPS服务 - --entrypoints=Name:https Address::443 TLS # HTTPS服务监听在443端口 - --entrypoints=Name:http Address::80 Redirect.EntryPoint:https # HTTP监听在80端口，并将流量重定向至https - --acme # 开启证书验证 - --acme.acmeLogging=true # 打开日志，方便排错 - --acme.email=1415228958@qq.com # 邮箱 - --acme.entryPoint=https # 证书类型 - --acme.storage=/tmp/acme.json # 证书申请临时文件 - --acme.dnschallenge # 域名验证方式 - --acme.dnschallenge.provider=alidns # 域名提供商 - --acme.dnschallenge.delaybeforecheck=5 # 验证域名延时 - --acme.httpchallenge.entrypoint=http # 验证域名时使用的协议 - --acme.domains=*.fanfengqiang.com # 要申请证书的域名 - --acme.domains=*.5ik8s.com # 要申请证书的域名---kind: ServiceapiVersion: v1metadata: name: traefik-ingress-service namespace: kube-systemspec: selector: k8s-app: traefik-ingress-lb ports: - protocol: TCP port: 80 name: web - protocol: TCP port: 443 name: https - protocol: TCP port: 8080 name: admin 4. 安装123kubectl label nodes node2 edge="true"kubectl apply -f traefik-rbac.yamlkubectl apply -f traefik-ds.yaml 5. 测试5.1. 修改测试文件需改ui.yaml 123456789101112131415161718192021222324252627282930---apiVersion: v1kind: Servicemetadata: name: traefik-web-ui namespace: kube-systemspec: selector: k8s-app: traefik-ingress-lb ports: - name: web port: 80 targetPort: 8080---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: traefik-web-ui namespace: kube-systemspec: rules: - host: traefik.5ik8s.com # 更改为自己的域名 http: paths: - path: / backend: serviceName: traefik-web-ui servicePort: web# tls: # 如果上面没有开启lets encrypt也可以使用此项声明证书# - secretName: 5ik8s.com 5.2. 测试1kubectl apply -f ui.yaml 5.3. 验证]]></content>
      <tags>
        <tag>kubernetes</tag>
        <tag>traefik</tag>
        <tag>ingress controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自己动手扩展K8S功能————为自定义资源CRD写一个控制器]]></title>
    <url>%2F2019%2F04%2F01%2F%E8%87%AA%E5%B7%B1%E5%8A%A8%E6%89%8B%E6%89%A9%E5%B1%95K8S%E5%8A%9F%E8%83%BD%E2%80%94%E2%80%94%E2%80%94%E2%80%94%E4%B8%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E8%B5%84%E6%BA%90CRD%E5%86%99%E4%B8%80%E4%B8%AA%E6%8E%A7%E5%88%B6%E5%99%A8%2F</url>
    <content type="text"><![CDATA[cert-controller作为一个苦逼的屌丝，我们是没有money去买TLS证书的，怎么办呢？当然是去找免费的证书签发机构啦！我们经常用的免费证书签发机构有Let’s encrypt.但证书有效期只有短短的90天，你说天天让人换证书是不是挺麻烦的，而且生成证书再导入K8S还需要手动完成。有没有一种方法让K8S自动的签署证书并保存为secret，并在证书快过期的时候自动更新？在我的一番Google之下还真找到一个项目cert-manager,但是！！！这个项目需要将要生成https证书的domain指向K8S的ingress，而且必须是一个公网地址（感兴趣的可以自行研究下）。而我的测试环境DNS是指向私网地址的!这不是蛋疼吗！怎么办？自己动手丰衣足食。 项目地址 功能介绍 安装cert-controller后，通过添加cert自定义资源，实现证书自动签发 将签发的证书自动保存到K8S的secret中 设置secret有效器，过期后重新签发证书，并自动更细secret 使用说明 创建自定义资源CRD 123kubectl apply -f https://raw.githubusercontent.com/fanfengqiang/cert-controller/master/deploy/cert-controller-rbac.yamlkubectl apply -f https://raw.githubusercontent.com/fanfengqiang/cert-controller/master/deploy/cert-controller-deploy.yamlkubectl apply -f https://raw.githubusercontent.com/fanfengqiang/cert-controller/master/deploy/crd.yaml 编写cert资源清单并应用 123456789101112131415cat &gt; cert.yaml &lt;&lt; EOFapiVersion: certcontroller.5ik8s.com/v1beta1kind: Certmetadata: name: example-certspec: secretName: cert-5ik8s.com domain: 5ik8s.com validityPeriod: 30 type: dns_ali env: Ali_Key: "XXXXXXXXXXXXXXXXXXXXXXXX" Ali_Secret: "XXXXXXXXXXXXXXXXXXXXXXX"EOFkubectl apply -f cert.yaml 参数定义 | 参数 | 含义 || :——————- | :——————————— || .metadata.name | cert资源的名字 || .spec.secretName | 生成的secret的名字 || .spec.domain | 生成证书的域名 || .spec.validityPeriod | secret的有效时长，单位天，范围1~89 || .spec.type | 域名托管商的标示 || .spec.env | 域名托管商API的accesskey和secret | 完整域名托管商的格式，accesskey和secret格式参见 手动构建123go get github.com/fanfengqiang/cert-controllercd $GOPATH/src/github.com/fanfengqiang/cert-controllerdocker build -t cert-controller:latest . 致谢本项目参考了如下两个项目 acme.sh sample-controller]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[源码编译kubernetes dashboard]]></title>
    <url>%2F2019%2F03%2F17%2F%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91kubernetes-dashboard%2F</url>
    <content type="text"><![CDATA[环境 os: centos7.6 golang: 1.11.4 jdk: 1.8 docker-ce: 18.06.3 node.js: v9.11.2 npm: 6.9.0 gulp: 3.9 安装编译环境安装开发包组1yum groupinstall "Development Tools" 安装docker参考 安装docker yum源123456# step 1: 安装必要的一些系统工具sudo yum install -y yum-utils device-mapper-persistent-data lvm2# Step 2: 添加软件源信息sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# Step 3: 更新并安装 Docker-CEsudo yum makecache fast 安装指定版本123456# 安装指定版本的Docker-CE:# Step 1: 查找Docker-CE的版本:yum list docker-ce.x86_64 --showduplicates | sort -ryum -y install docker-ce-18.06.3.ce-3.el7systemctl enable dockersystemctl start docker 安装golang下载golang安装程序1wget https://dl.google.com/go/go1.11.4.linux-amd64.tar.gz 解压1tar xvf go1.11.4.linux-amd64.tar.gz -C /usr/local/ 配置环境变量12345# cat /etc/profile.d/go.sh export GOROOT=/usr/local/goexport GOBIN=$GOROOT/binexport PATH=$GOBIN:$GOPATH/bin:$PATHexport GOPATH=/root/go/ 验证12source /etc/profile.d/go.sh go version 安装JDK下载JDK下载链接 解压1234tar xvf jdk-8u201-linux-x64.tar.gz -C /usr/local/cd /usr/local/ln -sv jdk1.8.0_201 jdkcd - 配置环境变量123456# cat /etc/profile.d/jdk.sh #set oracle jdk environmentexport JAVA_HOME=/usr/local/jdkexport JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 验证12source /etc/profile.d/jdk.sh java -version 安装node.js下载nvm安装程序1curl https://raw.githubusercontent.com/creationix/nvm/v0.30.2/install.sh | bash 安装node123456789101112131415source ~/.bashrc# 产看所有版本nvm ls-remote# 安装nvm install v9.11.2# 产看所有安装版本# nvm ls# 设置默认版本nvm alias default v9.11.2# 临时切换版本# nvm use v9.11.2 更新npm1npm install -g npm 验证12node -vnpm -v 安装gulp12npm install --global gulp-clinpm install -g gulp@3.9.0 验证1gulp -v 安装依赖库下载源码包下载地址 12345mkdir -p $GOPATH/src/github.com/kubernetes/cd $GOPATH/src/github.com/kubernetes/wget https://github.com/kubernetes/dashboard/archive/v1.10.1.tar.gztar xvf v1.10.1.tar.gzmv dashboard-1.10.1 dashboard 安装node modules12cd $GOPATH/src/github.com/kubernetes/dashboardnpm ci 运行安装脚本1npm run install 编译指定环境变量123export KUBE_DASHBOARD_APISERVER_HOST="http://127.0.0.1:8080"# 或者# export KUBE_DASHBOARD_KUBECONFIG="&lt;KUBECONFIG_FILE_PATH&gt;" 注: 环境变量 KUBE_DASHBOARD_KUBECONFIG 的优先级比KUBE_DASHBOARD_APISERVER_HOST的高 直接编译启动12345# 开启kubectl代理kubectl proxy# 启动服务gulp serve 编译二进制程序1gulp build 编译并打包为docker镜像1gulp docker-image:head 生成的镜像为：kubernetes/kubernetes-dashboard-amd64:head]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes TLS Bootstrapping配置]]></title>
    <url>%2F2019%2F03%2F11%2Fkubernetes-TLS-Bootstrapping%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[序前两天考试CKA遇到这样一道题目:添加一个node节点到集群中。这其实并不难，但由于不了解实验环境结构第一步就卡住了，结果就jj了（PS:此题8分,我最终成绩92）。对于一个具有完美主义的强迫症来说是不能接受自己的无知的，经过google和查阅文档终于搞明白了。 下面是考试题目（第22题） Set configuration context $ kubectl config use-context ik8sIn this task, you will configure a new Node, ik8s-node-0, to join a Kubernetes cluster as follows: Configure kubelet for automatic certificate rotation and ensure that both server and client CSRs are automatically approved and signed as appropnate via the use of RBAC. Ensure that the appropriate cluster-info ConfigMap is created and configured appropriately in the correct namespace so that future Nodes can easily join the cluster Your bootstrap kubeconfig should be created on the new Node at /etc/kubernetes/bootstrap-kubelet.conf (do not remove this file once your Node has successfully joined the cluster) The appropriate cluster-wide CA certificate is located on the Node at /etc/kubernetes/pki/ca.crt . You should ensure that any automatically issued certificates are installed to the node at /var/lib/kubelet/pki and that the kubeconfig file for kubelet will be rendered at /etc/kubernetes/kubelet.conf upon successful bootstrapping Use an additional group for bootstrapping Nodes attempting to join the cluster which should be called system:bootstrappers:cka:default-node-token Solution should start automatically on boot, with the systemd service unit file for kubelet available at /etc/systemd/system/kubelet.service To test your solution, create the appropriate resources from the spec file located at /opt/…./kube-flannel.yaml This will create the necessary supporting resources as well as the kube-flannel -ds DaemonSet . You should ensure that this DaemonSet is correctly deployed to the single node in the cluster. Hints: kubelet is not configured or running on ik8s-master-0 for this task, and you should not attempt to configure it. You will make use of TLS bootstrapping to complete this task. You can obtain the IP address of the Kubernetes API server via the following command $ ssh ik8s-node-0 getent hosts ik8s-master-0 The API server is listening on the usual port, 6443/tcp, and will only server TLS requests The kubelet binary is already installed on ik8s-node-0 at /usr/bin/kubelet . You will not need to deploy kube-proxy to the cluster during this task. You can ssh to the new worker node using $ ssh ik8s-node-0 You can ssh to the master node with the following command $ ssh ik8s-master-0 No further configuration of control plane services running on ik8s-master-0 is required You can assume elevated privileges on both nodes with the following command $ sudo -i Docker is already installed and running on ik8s-node-0 Question weight: 8% 题目的大意是node节点通过RBAC自动完成证书申请，master要自动同意node节点的申请CSR，剩下的全部是完成这个流程的细节要求（应该是为了跑成绩脚本的标准化） TLS bootstrap的流程kubelet需要申请那些证书集群启用RBAC后各组件之间的通信是基于TLS加密的，client和server需要通过证书中的CN，O来确定用户的user和group，因此client和server都需要持有有效证书 node节点的kubelet需要和master节点的apiserver通信，此时kubelet是一个client需要申请证书 node节点的kubelet启动为守住进程通过10250端口暴露自己的状态，此时kubelet是一个server需要申请证书 kubelet申请证书的步骤 集群产生一个低权账号用户组，并通过TOKEN进行认证 创建ClusterRole使其具有创建证书申请CSR的权限 给这个组添加ClusterRoleBinding，使得具有这个组的账号的kubelet具有上述权限 给添加ClusterRoleBinding，使得controller-manager自动同意上述两个证书的下发 调整 Controller Manager确保启动tokencleaner和bootstrapsigner（4中自动证书下发的功能） 基于上述TOKEN生成bootstrap.kubeconfig文件，并下发给node节点 node节点的kubelet拿着这个bootstrap.kubeconfig向master的apiserver发起CSR master自动同意并下发第一个证书 node记得点的kubelet自动拿着第一个证书与master的apiserver通信申请第二个证书 master自动同意并下发第二个证书 node节点加入集群 踩坑记大家可能看到了，要想完成证书申请需要TOKEN，而我就栽在了第一步。 通常我们搭建的集群有两种运行方式： 二进制部署以守住进程运行 kubeadm部署以静态POD运行 第一种方式：部署是通常写一个token.csv文件，apiserver启动时加上参数”–token-auth-file=/etc/kubernetes/token.csv” 16df3c701f979cee17732c30958745947,kubelet-bootstrap,10001,&quot;system:bootstrappers&quot; 上述”6df3c701f979cee17732c30958745947”为TOKEN值，”system:bootstrappers”为用户组 第二种方式：12345# 创建一个TOKEN，指定组为system:bootstrappers:cka:default-node-tokenkubeadm token create# 列出所有TOKENkubeadm token list 坑：既然已经知道了如何获取TOKEN那就开始吧，docker ps有内容apisever，controller-manager,scheduler都是静态POD，第一种方式行不通那就第二种，kubeadm！纳尼！！！没有kubeadm命令！ 此时我的内心是崩溃的第一步都进行不下去，无奈这题就栽在这里了。 考后在google的帮忙下找到了第三种生成TOKEN的方法 第三种方式：可以创建类型为”bootstrap.kubernetes.io/token”的secret，参见官方文档 12echo "$(head -c 6 /dev/urandom | md5sum | head -c 6)"."$(head -c 16 /dev/urandom | md5sum | head -c 16)"# d08961.c704b892a7c0a5f6 执行上述命令得到一个TOKEN值”d08961.c704b892a7c0a5f6” 这个 d08961.c704b892a7c0a5f6 就是生成的 Bootstrap Token，保存好 token，因为后续要用；关于这个 token 解释如下: Token 必须满足 [a-z0-9]{6}\.[a-z0-9]{16} 格式；以 . 分割，前面的部分被称作 Token ID ， Token ID 并不是 “机密信息”，它可以暴露出去；相对的后面的部分称为 Token Secret ，它应该是保密的。 下面我们将这个TOKEN做成secret： 1234567891011121314151617181920212223242526apiVersion: v1kind: Secretmetadata: # Name MUST be of form "bootstrap-token-&lt;token id&gt;" name: bootstrap-token-d08961 namespace: kube-system# Type MUST be 'bootstrap.kubernetes.io/token'type: bootstrap.kubernetes.io/tokenstringData: # Human readable description. Optional. description: "The default bootstrap token generated by 'kubeadm init'." # Token ID and secret. Required. token-id: d08961 token-secret: c704b892a7c0a5f6 # Expiration. Optional. expiration: 2020-03-10T03:22:11Z # Allowed usages. usage-bootstrap-authentication: "true" usage-bootstrap-signing: "true" # Extra groups to authenticate the token as. Must start with "system:bootstrappers:" auth-extra-groups: system:bootstrappers:cka:default-node-token 12kubectl create -n kube-system -f bootstrap-secret.yamlkubectl get secrets -n kube-system 完整答案剩下的就是接住上面部分按要求做完 创建ClusterRole参见官方文档 123456789101112131415161718192021222324252627282930313233343536# A ClusterRole which instructs the CSR approver to approve a user requesting# node client credentials.kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:certificates.k8s.io:certificatesigningrequests:nodeclientrules:- apiGroups: ["certificates.k8s.io"] resources: ["certificatesigningrequests/nodeclient"] verbs: ["create"]---# A ClusterRole which instructs the CSR approver to approve a node renewing its# own client credentials.kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclientrules:- apiGroups: ["certificates.k8s.io"] resources: ["certificatesigningrequests/selfnodeclient"] verbs: ["create"]---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:node-bootstrapperrules:- apiGroups: - certificates.k8s.io resources: - certificatesigningrequests verbs: - create - get - list - watch 有些role系统已经内置了，如果提示报错直接忽略就行了 创建ClusterRoleBinding12345678910111213141516171819202122232425apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: cka:kubelet-bootstraproleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:node-bootstrappersubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:bootstrappers:cka:default-node-token---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: cka:node-autoapprove-bootstraproleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:nodeclientsubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:bootstrappers:cka:default-node-token 调整 Controller Manage12vim /etc/kubernetes/manifests/kube-controller-manager.yaml# 确保配置文件中有这行"--controllers=*,bootstrapsigner,tokencleaner" 生成bootstrap.kubeconfig文件我这里的apiserver的地址为”https://192.168.65.26:6443&quot; 1234567891011121314151617# 设置集群参数kubectl config set-cluster cka \ --certificate-authority=/etc/kubernetes/pki/ca.crt \ --embed-certs=true \ --server=https://192.168.65.26:6443 \ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf# 设置客户端认证参数kubectl config set-credentials system:bootstrap:d08961 \ --token=d08961.c704b892a7c0a5f6 \ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf# 设置上下文参数kubectl config set-context default \ --cluster=cka \ --user=system:bootstrap:d08961 \ --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf# 设置默认上下文kubectl config use-context default --kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf 将其拷贝到node节点 1scp /etc/kubernetes/bootstrap-kubelet.conf node3:/etc/kubernetes/bootstrap-kubelet.conf 配置node节点的kubelet编辑kubelet.config 1vim /etc/kubernetes/kubelet.config 123456789101112kind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 192.168.65.28port: 10250readOnlyPort: 10255cgroupDriver: cgroupfsclusterDNS: ["10.96.0.10"]clusterDomain: cluster.local.failSwapOn: falseauthentication: anonymous: enabled: true 编辑kubelet.service 1vim /etc/systemd/system/kubelet.service 123456789101112131415161718192021[Unit]Description=Kubernetes KubeletAfter=docker.serviceRequires=docker.service[Service]#EnvironmentFile=/k8s/kubernetes/cfg/kubeletExecStart=/usr/bin/kubelet \--logtostderr=true \--v=4 \--hostname-override=192.168.65.28 \--kubeconfig=/etc/kubernetes/kubelet.kubeconfig \--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \--config=/etc/kubernetes/kubelet.config \--cert-dir=/etc/kubernetes/pki \--pod-infra-container-image=k8s.gcr.io/pause:3.1Restart=on-failureKillMode=process[Install]WantedBy=multi-user.target 123systemctl daemon-reload systemctl restart kubelet.servicesystemctl status kubelet.service]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openldap安装配置]]></title>
    <url>%2F2019%2F02%2F01%2Fopenldap%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1. LDAP介绍轻型目录访问协议（英文：Lightweight Directory Access Protocol，缩写：LDAP）是一个开放的，中立的，工业标准的应用协议，通过IP协议提供访问控制和维护分布式信息的目录信息。 目录是一个为查询、浏览和搜索而优化的专业分布式数据库，它呈树状结构组织数据，就好象Linux/Unix系统中的文件目录一样。目录数据库和关系数据库不同，它有优异的读性能，但写性能差，并且没有事务处理、回滚等复杂功能，不适于存储修改频繁的数据。所以目录天生是用来查询的，就好象它的名字一样。 2. 常用组件2.1. openLDAPOpenLDAP是轻型目录访问协议（Lightweight Directory Access Protocol，LDAP）的自由和开源的实现，在其OpenLDAP许可证下发行，并已经被包含在众多流行的Linux发行版中。 2.2. phpLDAPadmin一个开源的ldap web管理界面，语言用PHP编写，依赖PHP环境 2.3. Self Service Passwordldap的密码需改和密码找回 3. 安装3.1. 环境OS:Centos7.6已安装epel源 openldap-servers:2.4.44 3.2. openLDAP3.2.1. 安装软件包12# 软件都在base源yum install openldap openldap-servers openldap-clients -y 3.2.2. 复制数据库模板12cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIGchown ldap.ldap /var/lib/ldap/DB_CONFIG 3.2.3. 配置LDAPS证书（可选）由于ldap是明文的，如果外网访问需要TLS加密 3.2.3.1. 生成自签名证书（可选）接下来生成LDAP服务器的自签名证书，输入以下命令将在/etc/openldap/certs/目录下生成证书和私钥。 1openssl req -new -x509 -nodes -out /etc/openldap/certs/ldap.cert -keyout /etc/openldap/certs/ldap.key -days 365 3.2.3.2. 修改用户组1chown -R ldap:ldap /etc/openldap/certs/* 3.2.3.3. 修改配置文件123456vim /etc/openldap/slapd.conf# 添加如下三行#TLSCACertificateFile /etc/openldap/certs/ca.crt#TLSCertificateFile /etc/openldap/certs/ldapserver.crt#TLSCertificateKeyFile /etc/openldap/certs/ldapkey.pem 1vim /etc/sysconfig/slapd 3.2.4. 启动服务12systemctl enable slapdsystemctl start slapd 3.2.5. 查看端口是否启动 389是ldap端口，636是ldaps端口（只有启动TLS加密功能才会监听此端口） 3.2.6. 生成openldap的管理员密码123# -s后面跟的是明文密码，结果是密文密码slappasswd -s 5ik8s #&#123;SSHA&#125;dC1q0RSZCTUHbJGzDFHVsghsQk3cr9js 3.2.7. 将管理员密码导入到 LDAP配置文件3.2.7.1. 编辑文件chrootpw.ldif1234dn: olcDatabase=&#123;0&#125;config,cn=configchangetype: modifyadd: olcRootPWolcRootPW: &#123;SSHA&#125;dC1q0RSZCTUHbJGzDFHVsghsQk3cr9js 3.2.7.2. 执行如下命令1ldapadd -Y EXTERNAL -H ldapi:/// -f chrootpw.ldif 3.2.8. 向 LDAP 中导入一些基本的 Schema123ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/cosine.ldif ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/nis.ldif ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/inetorgperson.ldif 3.2.9. 配置 LDAP 的根域及其管理域以 dc=5ik8s,dc=com 为例 3.2.9.1. 编辑文件chdomain.ldif12345678910111213141516171819202122232425262728dn: olcDatabase=&#123;1&#125;monitor,cn=configchangetype: modifyreplace: olcAccessolcAccess: &#123;0&#125;to * by dn.base="gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth" read by dn.base="cn=admin,dc=5ik8s,dc=com" read by * nonedn: olcDatabase=&#123;2&#125;hdb,cn=configchangetype: modifyreplace: olcSuffixolcSuffix: dc=5ik8s,dc=comdn: olcDatabase=&#123;2&#125;hdb,cn=configchangetype: modifyreplace: olcRootDNolcRootDN: cn=admin,dc=5ik8s,dc=comdn: olcDatabase=&#123;2&#125;hdb,cn=configchangetype: modifyadd: olcRootPWolcRootPW: &#123;SSHA&#125;dC1q0RSZCTUHbJGzDFHVsghsQk3cr9jsdn: olcDatabase=&#123;2&#125;hdb,cn=configchangetype: modifyadd: olcAccessolcAccess: &#123;0&#125;to attrs=userPassword,shadowLastChange by dn="cn=admin,dc=5ik8s,dc=com" write by anonymous auth by self write by * noneolcAccess: &#123;1&#125;to dn.base="" by * readolcAccess: &#123;2&#125;to * by dn="cn=admin,dc=5ik8s,dc=com" write by * read 3,2,9,2， 执行命令1ldapmodify -Y EXTERNAL -H ldapi:/// -f chdomain.ldif 3.2.10. 创建组织创建一个叫做 5ik8s org. 的组织，并在其下创建一个 admin 的组织角色（该组织角色内的用户具有管理整个 LDAP 的权限）和 People 和 Group 两个组织单元 3.2.10.1. 编辑文件basedomain.ldif123456789101112131415161718dn: dc=5ik8s,dc=comobjectClass: topobjectClass: dcObjectobjectClass: organizationo: 5ik8s org.dc: 5ik8sdn: cn=admin,dc=5ik8s,dc=comobjectClass: organizationalRolecn: admindn: ou=People,dc=5ik8s,dc=comobjectClass: organizationalUnitou: Peopledn: ou=Group,dc=5ik8s,dc=comobjectClass: organizationalRolecn: Group 3.2.10.2. 执行命令12ldapadd -x -D cn=admin,dc=5ik8s,dc=com -f basedomain.ldif -w 5ik8s# -W后面跟的是密码，如不加则需要交互式输入密码 3.3. phpLDAPadmin3.3.1. 安装12# phpldapadmin在epel源yum -y install phpldapadmin 默认会依赖安装httpd，php。（php默认以模块方式安装） 3.3.2. 配置3.3.2.1. 修改apache为外网可访问1vim /etc/httpd/conf.d/phpldapadmin.conf 局域网内部也可以改为 Require local Require ip 192.168.0.0/16 #在这里添加允许访问的IP 3.3.2.2. 配置登录方式1234vim /etc/phpldapadmin/config.php# 修改第397行# $servers-&gt;setValue('login','attr','uid'); #改为下行# $servers-&gt;setValue('login','attr','dn'); 3.3.3. 启动服务12systemctl enable httpdsystemctl start httpd 3.4. Self Service Password3.4.1 安装rpm，yum两种安装方式二选一 3.4.1.1. yum安装配置yum源1vi /etc/yum.repos.d/ltb-project.repo 123456[ltb-project-noarch]name=LTB project packages (noarch)baseurl=https://ltb-project.org/rpm/$releasever/noarchenabled=1gpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-LTB-project 导入repository key1rpm --import https://ltb-project.org/wiki/lib/RPM-GPG-KEY-LTB-project 安装12yum repolistyum install self-service-password 3.4.1.2. RPM安装下载软件包https://ltb-project.org/download#self_service_password 导入repository key1rpm --import https://ltb-project.org/wiki/lib/RPM-GPG-KEY-LTB-project 安装1yum localinstall self-service-password-VERSION.noarch.rpm 3.4.2. 配置3.4.2.1. 配置web服务Apache和nginx二选一（默认选择Apache）参考 配置Apache1vim /etc/httpd/conf.d/self-service-password.conf 12345678910111213141516171819202122232425Alias /ssp /usr/local/self-service-password &lt;Directory /usr/local/self-service-password&gt; AllowOverride None &lt;IfVersion &gt;= 2.3&gt; Require all granted &lt;/IfVersion&gt; &lt;IfVersion &lt; 2.3&gt; Order Deny,Allow Allow from all &lt;/IfVersion&gt; DirectoryIndex index.php AddDefaultCharset UTF-8&lt;/Directory&gt; &lt;Directory /usr/local/self-service-password/scripts&gt; AllowOverride None &lt;IfVersion &gt;= 2.3&gt; Require all denied &lt;/IfVersion&gt; &lt;IfVersion &lt; 2.3&gt; Order Deny,Allow Deny from all &lt;/IfVersion&gt;&lt;/Directory&gt; 配置Nginx（可选）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758server &#123;listen 80; root /var/www/html;index index.php index.html index.htm; # Make site accessible from http://localhost/server_name _; # Disable sendfile as per https://docs.vagrantup.com/v2/synced-folders/virtualbox.htmlsendfile off; gzip on; gzip_comp_level 6; gzip_min_length 1000; gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript application/javascript text/x-js; gzip_vary on; gzip_proxied any; gzip_disable "MSIE [1-6]\.(?!.*SV1)"; # Add stdout logging error_log /dev/stdout warn;access_log /dev/stdout main; # pass the PHP scripts to FastCGI server listening on socket#location ~ \.php &#123; fastcgi_pass unix:/var/run/php-fpm.socket; fastcgi_split_path_info ^(.+\.php)(/.+)$; fastcgi_param PATH_INFO $fastcgi_path_info; fastcgi_param PATH_TRANSLATED $document_root$fastcgi_path_info; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_index index.php; fastcgi_read_timeout 600; include fastcgi_params;&#125; error_page 404 /404.html; location = /404.html &#123; root /usr/share/nginx/html; internal;&#125; # deny access to . files, for security#location ~ /\. &#123; log_not_found off; deny all;&#125; location ~ /scripts &#123; log_not_found off; deny all;&#125; &#125; 3.4.2.2. 配置ssp1vim /usr/share/self-service-password/conf/config.inc.php 12345678910111213141516171819202122232425262728293031# 修改如下值$ldap_url = "ldap://127.0.0.1:389";$ldap_binddn = "cn=admin,dc=5ik8s,dc=com";$ldap_bindpw = "5ik8s"; # 换成你自己的密码$ldap_base = "dc=5ik8s,dc=com";# 设置密码规则$hash = "SSHA";$pwd_min_length = 8;$pwd_max_length = 15;$pwd_min_lower = 1;$pwd_min_upper = 1;$pwd_min_digit = 1;# 不使用问题找回密码功能$use_questions = false;$mail_from = "ffqmail@163.com";$notify_on_change = true;$mail_smtp_host = 'smtp.163.com';$mail_smtp_auth = true;$mail_smtp_user = 'ffqmail@163.com';$mail_smtp_pass = 'XXXXXX'; # 换成你自己的密码$mail_smtp_port = 25;$mail_smtp_timeout = 30;# 不使用短信找回密码功能$use_sms = false;# 加密字符串，随机生成一个替换掉下面的值$keyphrase = "cufyvhbjyfytcug5yuh89"; 3.4.3. 重启服务1systemctl start httpd 4. 基本概念4.1. Entry条目，也叫记录项，是LDAP中最基本的颗粒，就像字典中的词条，或者是数据库中的记录。通常对LDAP的添加、删除、更改、检索都是以条目为基本对象的。 dn：每一个条目都有一个唯一的标识名（distinguished Name ，DN），如上图中一个 dn：”cn=baby,ou=marketing,ou=people,dc=mydomain,dc=org” 。通过DN的层次型语法结构，可以方便地表示出条目在LDAP树中的位置，通常用于检索。 rdn：一般指dn逗号最左边的部分，如cn=baby。它与RootDN不同，RootDN通常与RootPW同时出现，特指管理LDAP中信息的最高权限用户。 Base DN：LDAP目录树的最顶部就是根，也就是所谓的“Base DN”，如”dc=mydomain,dc=org”。 4.2. Attribute每个条目都可以有很多属性（Attribute），比如常见的人都有姓名、地址、电话等属性。每个属性都有名称及对应的值，属性值可以有单个、多个，比如你有多个邮箱。 属性 别名 语法 描述 值(举例) commonName cn Directory String 姓名 sean surname sn Directory String 姓 Chow organizationalUnitName ou Directory String 单位（部门）名称 IT_SECTION organization o Directory String 组织（公司）名称 example telephoneNumber Telephone Number 电话号码 110 4.3. ObjectClass对象类是属性的集合，LDAP预想了很多人员组织机构中常见的对象，并将其封装成对象类。比如人员（person）含有姓（sn）、名（cn）、电话(telephoneNumber)、密码(userPassword)等属性，单位职工(organizationalPerson)是人员(person)的继承类，除了上述属性之外还含有职务（title）、邮政编码（postalCode）、通信地址(postalAddress)等属性。 4.4. Schema对象类（ObjectClass）、属性类型（AttributeType）、语法（Syntax）分别约定了条目、属性、值，他们之间的关系如下图所示。所以这些构成了模式(Schema)——对象类的集合。条目数据在导入时通常需要接受模式检查，它确保了目录中所有的条目数据结构都是一致的。 4.5. backend &amp; databaseldap的后台进程slapd接收、响应请求，但实际存储数据、获取数据的操作是由Backends做的，而数据是存放在database中，所以你可以看到往往你可以看到backend和database指令是一样的值如 bdb 。一个 backend 可以有多个 database instance，但每个 database 的 suffix 和 rootdn 不一样。 4.6. TLS &amp; SASL分布式LDAP 是以明文的格式通过网络来发送信息的，包括client访问ldap的密码（当然一般密码已然是二进制的），SSL/TLS 的加密协议就是来保证数据传送的保密性和完整性。 SASL （Simple Authenticaion and Security Layer）简单身份验证安全框架，它能够实现openldap客户端到服务端的用户验证，也是ldapsearch、ldapmodify这些标准客户端工具默认尝试与LDAP服务端认证用户的方式（前提是已经安装好 Cyrus SASL）。SASL有几大工业实现标准：Kerveros V5、DIGEST-MD5、EXTERNAL、PLAIN、LOGIN。 4.7. LDIFLDIF（LDAP Data Interchange Format，数据交换格式）是LDAP数据库信息的一种文本格式，用于数据的导入导出，每行都是“属性: 值”对 5. 备份与恢复5.1. 备份5.1.1. 使用slapcat指令备份12345678910111213slapcat -v -l openldap-backup-tmp.ldif# 此命令导出的数据过于详细有些数据无法直接导入需要进行数据整理cat &gt; openldap-backup.synax &lt;&lt; EOF/^creatorsName: /d/^createTimestamp: /d/^modifiersName: /d/^modifyTimestamp: /d/^structuralObjectClass: /d/^entryUUID: /d/^entryCSN: /dEOFcat openldap-backup-tmp.ldif | sed -f openldap-backup.synax &gt; openldap-backup.ldifrm -f openldap-backup-tmp.ldif 5.1.2. 备份数据目录数据目录为/etc/openldap/slapd.d 备份可以用cp或者tar命令进行备份 1tar czvf openldap-slapd-`date +%Y_%m_%d_%H_%M`.tar /etc/openldap/slapd.d 恢复的时候直接覆盖原目录即可 5.1.3. 使用ldapsearch备份1ldapsearch -LLL -w 5ik8s -x -H ldap://127.0.0.1 -D "cn=admin,dc=5ik8s,dc=com" -b "dc=5ik8s,dc=com" &gt;openldap-backup.ldif 5.2. 恢复5.2.1. 先删除所有条目1ldapdelete -x -D "cn=admin,dc=域名,dc=com" -w "密码" -r "dc=域名,dc=com" 5.2.2. 查看1ldapsearch -x -LLL 5.2.3. 恢复1ldapadd -H ldap://127.0.0.1 -x -D "用户" -f openldap-backup.ldif -w 密码 5.3. 恢复常见错误1234 ldap_add: Constraint violation (10)additional info: structuralObjectClass: no user modification allowed slapcat备份出来的ldapback.ldif中有系统自动生成的系统信息不能导入需要清除 6. 常用命令ldapadd-x 进行简单认证-D 用来绑定服务器的DN-h 目录服务的地址-w 绑定DN的密码-f 使用ldif文件进行条目添加的文件例子 ldapadd -x -D “cn=root,dc=starxing,dc=com” -w secret -f /root/test.ldif ldapadd -x -D “cn=root,dc=starxing,dc=com” -w secret (这样写就是在命令行添加条目) ldapsearch-x 进行简单认证 -D 用来绑定服务器的DN -w 绑定DN的密码 -b 指定要查询的根节点 -H 制定要查询的服务器 ldapsearch -x -D “cn=admin,dc=5ik8s,dc=com” -b “dc=5ik8s,dc=com” -w secret 使用简单认证，用 “cn=root,dc=starxing,dc=com” 进行绑定，要查询的根是 “dc=starxing,dc=com”。这样会把绑定的用户能访问”dc=starxing,dc=com”下的所有数据显示出来。 ldapsearch -x -W -D “cn=administrator,cn=users,dc=osdn,dc=zzti,dc=edu,dc=cn” -b “cn=administrator,cn=users,dc=osdn,dc=zzti,dc=edu,dc=cn” -h troy.osdn.zzti.edu.cn ldapsearch -b “dc=canon-is,dc=jp” -H ldaps://192.168.0.92:636 ldapdeleteldapdelete -x -D “cn=Manager,dc=test,dc=com” -w secret “uid=test1,ou=People,dc=test,dc=com” ldapdelete -x -D ‘cn=root,dc=it,dc=com’ -w secert ‘uid=zyx,dc=it,dc=com’ 这样就可以删除’uid=zyx,dc=it,dc=com’记录了，应该注意一点，如果o或ou中有成员是不能删除的。 ldappasswd-x 进行简单认证-D 用来绑定服务器的DN-w 绑定DN的密码-S 提示的输入密码-s pass 把密码设置为pass-a pass 设置old passwd为pass-A 提示的设置old passwd-H 是指要绑定的服务器-I 使用sasl会话方式 #ldappasswd -x -D ‘cm=root,dc=it,dc=com’ -w secret ‘uid=zyx,dc=it,dc=com’ -SNew password:Re-enter new password: 就可以更改密码了，如果原来记录中没有密码，将会自动生成一个userPassword。 ldapmodify-a 添加新的条目.缺省的是修改存在的条目.-C 自动追踪引用.-c 出错后继续执行程序并不中止.缺省情况下出错的立即停止.比如如果你的ldif 文 件内的某个条目在数据库内并不存在,缺省情况下程序立即退出,但如果使用了该参数,程 序忽略该错误继续执行.-n 用于调试到服务器的通讯.但并不实际执行搜索.服务器关闭时,返回错误；服务器 打开时,常和-v 参数一起测试到服务器是否是一条通路.-v 运行在详细模块.在标准输出中打出一些比较详细的信息.比如:连接到服务器的 ip 地址和端口号等.-M[M] 打开manage DSA IT 控制. -MM 把该控制设置为重要的.-f file 从文件内读取条目的修改信息而不是从标准输入读取.-x 使用简单认证.-D binddn 指定搜索的用户名(一般为一dn 值).-W 指定了该参数,系统将弹出一提示入用户的密码.它和-w 参数相对使用.-w bindpasswd 直接指定用户的密码. 它和-W 参数相对使用.-H ldapuri 指定连接到服务器uri(ip 地址和端口号,常见格式为 ldap://hostname:port).如果使用了-H 就不能使用-h 和-p 参数.-h ldaphost 指定要连接的主机的名称/ip 地址.它和-p 一起使用.-p ldapport 指定要连接目录服务器的端口号.它和-h 一起使用. 如果使用了-h 和-p 参数就不能使用-H 参数.-Z[Z] 使用StartTLS 扩展操作.如果使用-ZZ,命令强制使用StartTLS 握手成功.-V 启用证书认证功能,目录服务器使用客户端证书进行身份验证,必须与-ZZ 强制启用 TLS 方式配合使用,并且匿名绑定到目录服务器.-e 设置客户端证书文件,例: -e cert/client.crt-E 设置客户端证书私钥文件,例: -E cert/client.key #ldapmodify -x -D “cn=root,dc=it,dc=com” -W -f modify.ldif 将modify.ldif中的记录更新原有的记录。]]></content>
      <categories>
        <category>openldap</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>openldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes HA master集群搭建]]></title>
    <url>%2F2019%2F01%2F26%2Fkubernetes-HA-master%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1. 配置基础环境参见 kubernetesv1.13.2安装 其它要求：master大于等于2台（推荐3台），node节点大于等于1台 本文配置： hostname role ip master-1 master 192.168.65.51 master-2 master 192.168.65.52 master-3 master 192.168.65.53 vip 192.168.65.60 node-1 node 192.168.65.54 node-2 node 192.168.65.55 2. 安装kubernetes HA集群2.1. 关于kubernetes镜像由于默认配置的镜像仓库无法直接到达，且安装节点较多，本示例需要采用docker代理具体方法参见 kubernetesv1.13.2安装 2.2. master节点2.2.1. 调整内核参数123456789#写入配置文件cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confvm.swappiness = 0net.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF#生效配置文件sysctl -p /etc/sysctl.d/k8s.conf 配置VIP&amp;keepalive123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# 在masters上执行MASTERS_IP="192.168.65.51 192.168.65.52 192.168.65.53"VIP="192.168.65.60"PRIORITY=(100 80 60)STATE=("MASTER" "BACKUP" "BACKUP")NET_IF="eth0"for index in 0 1 2; do ip=$&#123;MASTERS_IP[$&#123;index&#125;]&#125; ssh root@$&#123;ip&#125; "yum install -y keepalived" echo """global_defs &#123; router_id LVS_DEVEL&#125;vrrp_instance VI_1 &#123; state $&#123;STATE[$&#123;index&#125;]&#125; interface $&#123;NET_IF&#125; virtual_router_id 80 priority $&#123;PRIORITY[$&#123;index&#125;]&#125; advert_int 1 authentication &#123; auth_type PASS auth_pass just0kk &#125; virtual_ipaddress &#123; $&#123;VIP&#125; &#125;&#125;virtual_server $&#123;VIP&#125; 6443 &#123; delay_loop 6 lb_algo loadbalance lb_kind DR nat_mask 255.255.255.0 persistence_timeout 0 protocol TCP real_server 192.168.65.51 6443 &#123; weight 1 SSL_GET &#123; url &#123; path /healthz status_code 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125; real_server 192.168.65.52 6443 &#123; weight 1 SSL_GET &#123; url &#123; path /healthz status_code 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125; real_server 192.168.65.53 6443 &#123; weight 1 SSL_GET &#123; url &#123; path /healthz status_code 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125;&#125;""" &gt; ~/keepalived-$&#123;index&#125;.conf scp ~/keepalived-$&#123;index&#125;.conf $&#123;ip&#125;:/etc/keepalived/keepalived.conf ssh $&#123;ip&#125; """ systemctl stop keepalived systemctl enable keepalived systemctl start keepalived """done 2.2.2. 初始化master-1节点12345678910111213141516echo """apiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfigurationkubernetesVersion: v1.13.2controlPlaneEndpoint: "192.168.65.60:6443"apiServer: certSANs: - 192.168.65.51 - 192.168.65.52 - 192.168.65.53 - 192.168.65.60networking: # This CIDR is a Calico default. Substitute or remove for your CNI provider. podSubnet: 10.244.0.0/16""" &gt; /etc/kubernetes/kubeadm-config.yamlkubeadm init --config /etc/kubernetes/kubeadm-config.yaml 2.2.3. 拷贝证书到其它master节点123456789101112131415# 在master-1上执行IPS="192.168.65.52 192.168.65.53"for ip in $IPS; do ssh $ip "mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube/" scp /etc/kubernetes/pki/ca.crt $ip:/etc/kubernetes/pki/ca.crt scp /etc/kubernetes/pki/ca.key $ip:/etc/kubernetes/pki/ca.key scp /etc/kubernetes/pki/sa.key $ip:/etc/kubernetes/pki/sa.key scp /etc/kubernetes/pki/sa.pub $ip:/etc/kubernetes/pki/sa.pub scp /etc/kubernetes/pki/front-proxy-ca.crt $ip:/etc/kubernetes/pki/front-proxy-ca.crt scp /etc/kubernetes/pki/front-proxy-ca.key $ip:/etc/kubernetes/pki/front-proxy-ca.key scp /etc/kubernetes/pki/etcd/ca.crt $ip:/etc/kubernetes/pki/etcd/ca.crt scp /etc/kubernetes/pki/etcd/ca.key $ip:/etc/kubernetes/pki/etcd/ca.key scp /etc/kubernetes/admin.conf $ip:/etc/kubernetes/admin.conf scp /etc/kubernetes/admin.conf $ip:~/.kube/configdone 2.2.4. 添加其它master到集群123456# 在master-1上执行JOIN_CMD=`kubeadm token create --print-join-command`IPS="192.168.65.52 192.168.65.53"for ip in $IPS; do ssh $&#123;ip&#125; "$&#123;JOIN_CMD&#125; --experimental-control-plane"done 2.2.5. 配置kubectl客户端123rm -rf ~/.kubemkdir ~/.kube -pcp /etc/kubernetes/admin.conf ~/.kube/ 2.2.6. 添加网络附件（Addons）网络插件使用fannel 1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.3. node节点添加至集群网络 1kubeadm join 192.168.65.60:6443 --token 7gu7nv.19uizr187qk5vy9q --discovery-token-ca-cert-hash sha256:9bafc6ccf27bcfba9620aa2e285de1434eacbc09337063cdcca6b56cda13270f 3. 验证参见 kubernetesv1.13.2安装 4. 一键安装脚本4.1. 初始化脚本pre_install.sh123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104#!/bin/bash########################### SETING ############################################# 所有节点密码必须一致PASSWORD="666666"# 设置master节点IP地址（以空格分割,第一个必须是本机IP）MASTERS_IP="192.168.65.51 192.168.65.52 192.168.65.53"# 设置node节点IP地址（可以设置多个以空格分割）NODES_IP="192.168.65.54 192.168.65.55"# 设置主机名后缀（默认为空）HOSTNAME_DOMIN=""############################ FUNCTIONS ########################################config_nopasswd_login() &#123;# 首次登录不用输yessed -i.bak 's/.*StrictHostKeyChecking.*/StrictHostKeyChecking no/' /etc/ssh/ssh_config# 生成秘钥对if [[ ! -f ~/.ssh/id_rsa ]];then ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsafiyum -y install expectpassword=$1shift 1for host in $@ ;do /usr/bin/expect &lt;&lt;-EOFset time 30spawn ssh-copy-id root@$hostexpect &#123; "*yes/no" &#123; send "yes\r\n"; exp_continue&#125; "*password:" &#123; send "$password\r\n" &#125; &#125; expect eofEOFdone&#125;############################## MAIN LOOP ######################################MASTERS_IP=($&#123;MASTERS_IP&#125;)config_nopasswd_login $PASSWORD $&#123;MASTERS_IP[@]&#125; $NODES_IPfor host in `eval echo "$MASTERS_IP $NODES_IP"`;dossh $host """# 关闭防火墙systemctl disable firewalld.servicesystemctl stop firewalld.service# 关闭selinuxsetenforce 0sed -i.bak 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config# 关闭swapsed -ri '/^[^#]/s@(.*swap.*)@# \1@' /etc/fstabswapoff -a# 配置时间同步yum install -y ntpdatentpdate -u ntp.api.bz# 升级内核rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpmyum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -ygrub2-set-default 0"""donenum=1for host in $&#123;MASTERS_IP[@]&#125;;do echo $host master-$&#123;num&#125;$&#123;HOSTNAME_DOMIN&#125; &gt;&gt; /etc/hosts ssh $host "hostnamectl set-hostname master-$&#123;num&#125;$&#123;HOSTNAME_DOMIN&#125;" let num++done num=1for host in $NODES_IP;do echo $host node-$&#123;num&#125;$&#123;HOSTNAME_DOMIN&#125; &gt;&gt; /etc/hosts ssh $host "hostnamectl set-hostname node-$&#123;num&#125;$&#123;HOSTNAME_DOMIN&#125;" let num++done for host in `eval echo "$&#123;MASTERS_IP[@]&#125; $NODES_IP"`;do scp /etc/hosts root@$host:/etc/hostsdonefor host in $NODES_IP;do ssh $host "reboot"done for index in 1 2; do host=$&#123;MASTERS_IP[$&#123;index&#125;]&#125; ssh $host "reboot"donereboot 4.2. 安装脚本install.sh123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238#!/bin/bash########################### SETING ############################################# 设置master节点IP地址（以空格分割,第一个必须是本机IP）MASTERS_IP="192.168.65.51 192.168.65.52 192.168.65.53"# 设置apiserver的虚拟ipVIP="192.168.65.60"# 设置node节点IP地址（可以设置多个以空格分割）NODES_IP="192.168.65.54 192.168.65.55"# 配置kubrnetes版本VERSION="v1.13.2"# pod网络的cidr(默认为flannel)POD_CIDR="10.244.0.0/16"# 网卡名NET_IF="eth0"##############################################################################MASTERS_IP=($MASTERS_IP)for host in `eval echo "$&#123;MASTERS_IP[@]&#125; $NODES_IP"`;dossh $host """# 安装docker# step 1: 安装必要的一些系统工具yum install -y yum-utils device-mapper-persistent-data lvm2# Step 2: 添加软件源信息yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# Step 3: 更新并安装 Docker-CEyum makecache fastyum -y install docker-ce-18.06.1.ce-3.el7# 取消docker启动后iptables转发限制mkdir -p /lib/systemd/system/docker.service.dcat &gt;/lib/systemd/system/docker.service.d/iptables.conf &lt;&lt; EOF[Service]ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPTEOFmkdir -p /etc/docker/# cat &gt; /etc/docker/daemon.json &lt;&lt; EOF# &#123;# \"registry-mirrors\": [\"https://hub-mirror.c.163.com","https://docker.mirrors.ustc.edu.cn\"],# \"max-concurrent-downloads\": 20# &#125;# EOFcat &gt;/lib/systemd/system/docker.service.d/http_proxy.conf &lt;&lt; EOF[Service]Environment=\"HTTP_PROXY=http://192.168.65.26:8118/\" #代理服务器地址Environment=\"HTTPS_PROXY=http://192.168.65.26:8118/\" #httpsEnvironment=\"NO_PROXY=localhost,127.0.0.0/8,10.0.0.0/8,172.21.0.0/12,192.168.0.0/16,docker-registry.somecorporation.com,docker.mirrors.ustc.edu.cn,hub-mirror.c.163.com\" #哪些地址不需要走代理EOFsystemctl daemon-reloadsystemctl enable docker.servicesystemctl start docker.servicesystemctl restart docker.service# 安装kubeadm，kubectl，kubeletecat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFsetenforce 0yum install -y kubelet kubeadm kubectlsystemctl enable kubelet systemctl start kubeletsystemctl restart kubelet# 加载内核模块cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashipvs_modules=\"ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4\"for kernel_module in \$&#123;ipvs_modules&#125;; do /sbin/modinfo -F filename \$&#123;kernel_module&#125; &gt; /dev/null 2&gt;&amp;1 if [ \$? -eq 0 ]; then /sbin/modprobe \$&#123;kernel_module&#125; fidoneEOFchmod 755 /etc/sysconfig/modules/ipvs.modules bash /etc/sysconfig/modules/ipvs.moduleslsmod | grep ip_vsyum install -y ipvsadm# 配置内核参数cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confvm.swappiness = 0net.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF#生效配置文件sysctl -p /etc/sysctl.d/k8s.conf"""done############################################################################## 配置VIP&amp;keepalivePRIORITY=""prio=100STATE=""HEALTH_CHECK=""for ip in $&#123;MASTERS_IP[@]&#125;; do ssh root@$&#123;ip&#125; "yum install -y keepalived" HEALTH_CHECK=$&#123;HEALTH_CHECK&#125;""" real_server $&#123;ip&#125; 6443 &#123; weight 1 SSL_GET &#123; url &#123; path /healthz status_code 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125;"""if [[ $prio -eq "100" ]];then STATE="$&#123;STATE&#125; MASTER"else STATE="$&#123;STATE&#125; BACKUP"fiPRIORITY="$PRIORITY $&#123;prio&#125;"let prio=$&#123;prio&#125;-10doneSTATE=($STATE)PRIORITY=($PRIORITY)index="0"for ip in $&#123;MASTERS_IP[@]&#125;; do echo """global_defs &#123; router_id LVS_DEVEL&#125;vrrp_instance VI_1 &#123; state $&#123;STATE[$&#123;index&#125;]&#125; interface $&#123;NET_IF&#125; virtual_router_id 80 priority $&#123;PRIORITY[$&#123;index&#125;]&#125; advert_int 1 authentication &#123; auth_type PASS auth_pass just0kk &#125; virtual_ipaddress &#123; $&#123;VIP&#125; &#125;&#125;virtual_server $&#123;VIP&#125; 6443 &#123; delay_loop 6 lb_algo loadbalance lb_kind DR nat_mask 255.255.255.0 persistence_timeout 0 protocol TCP$&#123;HEALTH_CHECK&#125;&#125;""" &gt; ~/keepalived-$&#123;index&#125;.conf scp ~/keepalived-$&#123;index&#125;.conf $&#123;ip&#125;:/etc/keepalived/keepalived.conf ssh $&#123;ip&#125; """ systemctl stop keepalived systemctl enable keepalived systemctl start keepalived """ let index++done############################################################################### 初始化maser-1CERT_SANS=""" - $VIP"""for ip in $&#123;MASTERS_IP[@]&#125; ;doCERT_SANS=$&#123;CERT_SANS&#125;""" - $ip"""doneecho """apiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfigurationkubernetesVersion: $VERSIONcontrolPlaneEndpoint: "$&#123;VIP&#125;:6443"apiServer: certSANs:$&#123;CERT_SANS&#125;networking: # This CIDR is a Calico default. Substitute or remove for your CNI provider. podSubnet: $POD_CIDR""" &gt; /etc/kubernetes/kubeadm-config.yamlkubeadm init --config /etc/kubernetes/kubeadm-config.yaml# 在master-1上执行JOIN_CMD=`kubeadm token create --print-join-command`index="0"for ip in $&#123;MASTERS_IP[@]&#125;; do if [[ $&#123;index&#125; -ne "0" ]];then ssh $ip "mkdir -p /etc/kubernetes/pki/etcd; mkdir -p ~/.kube/" scp /etc/kubernetes/pki/ca.crt $ip:/etc/kubernetes/pki/ca.crt scp /etc/kubernetes/pki/ca.key $ip:/etc/kubernetes/pki/ca.key scp /etc/kubernetes/pki/sa.key $ip:/etc/kubernetes/pki/sa.key scp /etc/kubernetes/pki/sa.pub $ip:/etc/kubernetes/pki/sa.pub scp /etc/kubernetes/pki/front-proxy-ca.crt $ip:/etc/kubernetes/pki/front-proxy-ca.crt scp /etc/kubernetes/pki/front-proxy-ca.key $ip:/etc/kubernetes/pki/front-proxy-ca.key scp /etc/kubernetes/pki/etcd/ca.crt $ip:/etc/kubernetes/pki/etcd/ca.crt scp /etc/kubernetes/pki/etcd/ca.key $ip:/etc/kubernetes/pki/etcd/ca.key scp /etc/kubernetes/admin.conf $ip:/etc/kubernetes/admin.conf scp /etc/kubernetes/admin.conf $ip:~/.kube/config ssh $&#123;ip&#125; "$&#123;JOIN_CMD&#125; --experimental-control-plane" fi let index++donerm -rf ~/.kubemkdir ~/.kube -pcp /etc/kubernetes/admin.conf ~/.kube/curl -fsSL https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml|sed "s@10.244.0.0/16@$&#123;POD_CIDR&#125;@"| kubectl apply -f -# 添加node节点到集群for host in $NODES_IP ;do ssh root@$host "$join_cmd"doneecho "HAPPY K8S..."]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes v1.13.2安装]]></title>
    <url>%2F2019%2F01%2F25%2Fkubernetes%E7%AE%80%E5%8D%95%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[kubernetes简单安装1. 配置基础环境1.0. 配置要求centos7操作系统且已安装epel源 1.1. 关闭防火墙12systemctl disable firewalld.service systemctl stop firewalld.service 1.2. 关闭selinux12setenforce 0sed -i.bak 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 1.3. 关闭swap12sed -ri '/^[^#]/s@(.*swap.*)@# \1@' /etc/fstabswapoff -a 1.4. 配置时间同步123# 同步时间yum install -y ntpdatentpdate -u ntp.api.bz 1.5. 升级内核123456# 升级内核rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -ygrub2-set-default 0reboot 1.6. 安装docker123456789101112131415161718192021222324# step 1: 安装必要的一些系统工具sudo yum install -y yum-utils device-mapper-persistent-data lvm2# Step 2: 添加软件源信息sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# Step 3: 更新并安装 Docker-CEsudo yum makecache fastsudo yum -y install docker-ce-18.06.1.ce-3.el7# 注意：# 官方软件源默认启用了最新的软件，您可以通过编辑软件源的方式获取各个版本的软件包。例如官方并没有将测试版本的软件源置为可用，你可以通过以下方式开启。同理可以开启各种测试版本等。# vim /etc/yum.repos.d/docker-ce.repo# 将 [docker-ce-test] 下方的 enabled=0 修改为 enabled=1## 安装指定版本的Docker-CE:# Step 1: 查找Docker-CE的版本:# yum list docker-ce.x86_64 --showduplicates | sort -r# Loading mirror speeds from cached hostfile# Loaded plugins: branch, fastestmirror, langpacks# docker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stable# docker-ce.x86_64 17.03.1.ce-1.el7.centos @docker-ce-stable# docker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stable# Available Packages# Step2 : 安装指定版本的Docker-CE: (VERSION 例如上面的 17.03.0.ce.1-1.el7.centos)# sudo yum -y install docker-ce-[VERSION] 1.7. 取消docker启动后iptables转发限制12345678910111213141516mkdir -p /lib/systemd/system/docker.service.dcat &gt;/lib/systemd/system/docker.service.d/iptables.conf &lt;&lt; EOF[Service]ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPTEOFmkdir -p /etc/docker/cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; "registry-mirrors": ["https://hub-mirror.c.163.com", "https://docker.mirrors.ustc.edu.cn"], "max-concurrent-downloads": 20&#125;EOFsystemctl daemon-reloadsystemctl enable docker.servicesystemctl restart docker.service 1.8. 配置docker代理(可选)因为kubernetes组件容器无法直接获取，可以为docker配置http代理，本示例http代理地址http://192.168.65.26:8118。此步骤为可选项，可以通过docker hub镜像代理预拉镜像重新打tag完成部署 12345678cat &gt;/lib/systemd/system/docker.service.d/http_proxy.conf &lt;&lt; EOF[Service]Environment="HTTP_PROXY=http://192.168.65.26:8118/" #代理服务器地址Environment="HTTPS_PROXY=http://192.168.65.26:8118/" #httpsEnvironment="NO_PROXY=localhost,127.0.0.0/8,10.0.0.0/8,172.21.0.0/12,192.168.0.0/16,docker-registry.somecorporation.com,docker.mirrors.ustc.edu.cn,hub-mirror.c.163.com" #哪些地址不需要走代理EOFsystemctl daemon-reloadsystemctl restart docker.service 1.9. 安装kubeadm，kubectl，kubelete123456789101112cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFsetenforce 0yum install -y kubelet kubeadm kubectlsystemctl enable kubelet &amp;&amp; systemctl start kubelet 加载内核模块 1234567891011121314cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashipvs_modules="ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4"for kernel_module in \$&#123;ipvs_modules&#125;; do /sbin/modinfo -F filename \$&#123;kernel_module&#125; &gt; /dev/null 2&gt;&amp;1 if [ \$? -eq 0 ]; then /sbin/modprobe \$&#123;kernel_module&#125; fidoneEOFchmod 755 /etc/sysconfig/modules/ipvs.modulesbash /etc/sysconfig/modules/ipvs.moduleslsmod | grep ip_vsyum install -y ipvsadm 2. 安装kubernetes集群2.1. master节点2.1.1. 拉取kuberntes镜像（可选）如果没有配置docker代理，并且无法进行科学上网的情况下可以使用此步骤。（强烈建立配置代理） 拉取docker hub代理节点（https://hub.docker.com/u/mirrorgooglecontainers）的镜像并重新打tag 12345678910VERSION=v1.13.2images=`kubeadm --kubernetes-version=$VERSION config images list|awk -v FS='/' '&#123;print $2&#125;'`for image in `kubeadm --kubernetes-version=$VERSION config images list|awk -v FS='/' '&#123;print $2&#125;'` ;do docker pull mirrorgooglecontainers/$image docker tag mirrorgooglecontainers/$image k8s.gcr.io/$imagedoneimage=`kubeadm --kubernetes-version=$VERSION config images list|awk -v FS='/' '&#123;print $2&#125;'|grep coredns`docker pull coredns/$imagedocker tag coredns/$image k8s.gcr.io/$image 2.1.2. 调整内核参数123456789#写入配置文件cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confvm.swappiness = 0net.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF#生效配置文件sysctl -p /etc/sysctl.d/k8s.conf 2.1.3. 初始化master节点12345678910111213echo """apiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfigurationkubernetesVersion: v1.13.2networking: # This CIDR is a Calico default. Substitute or remove for your CNI provider. podSubnet: 10.244.0.0/16---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationmode: ipvs""" &gt; /etc/kubernetes/kubeadm-config.yamlkubeadm init --config /etc/kubernetes/kubeadm-config.yaml pod-network-cidr 10.224.0.0/16后面安装的网络插件对此有依赖 3.1.4. 配置kubectl客户端123rm -rf ~/.kubemkdir ~/.kube -pcp /etc/kubernetes/admin.conf ~/.kube/ 确保上述代码在使用kubectl命令前执行 2.1.5. 添加网络附件（Addons）1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 2.2. nodes节点添加至集群网络 1kubeadm join 10.2.2.1:6443 --token 7gu7nv.19uizr187qk5vy9q --discovery-token-ca-cert-hash sha256:9bafc6ccf27bcfba9620aa2e285de1434eacbc09337063cdcca6b56cda13270f 3. 验证在master上执行以下操作 12kubectl get componentstatuskubectl get nodes 4. 一键安装脚本4.1. 初始化脚本pre_install.sh123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#!/bin/bash########################### SETING ############################################# 所有节点密码必须一致PASSWORD="666666"# 设置master节点IP地址（只能写一个）MASTER_IP="192.168.65.101"# 设置node节点IP地址（可以设置多个以空格分割）NODES_IP="192.168.65.103"# 设置主机名后缀（默认为空）HOSTNAME_DOMIN=""############################ FUNCTIONS ########################################config_nopasswd_login() &#123;# 首次登录不用输yessed -i.bak 's/.*StrictHostKeyChecking.*/StrictHostKeyChecking no/' /etc/ssh/ssh_config# 生成秘钥对ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsayum -y install expectpassword=$1shift 1for host in $@ ;do /usr/bin/expect &lt;&lt;-EOFset time 30spawn ssh-copy-id root@$hostexpect &#123; "*yes/no" &#123; send "yes\r\n"; exp_continue&#125; "*password:" &#123; send "$password\r\n" &#125; &#125; expect eofEOFdone&#125;############################## MAIN LOOP ######################################config_nopasswd_login $PASSWORD $MASTER_IP $NODES_IPfor host in `eval echo "$MASTER_IP $NODES_IP"`;dossh $host """# 关闭防火墙systemctl disable firewalld.servicesystemctl stop firewalld.service# 关闭selinuxsetenforce 0sed -i.bak 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config# 关闭swapsed -ri '/^[^#]/s@(.*swap.*)@# \1@' /etc/fstabswapoff -a# 配置时间同步yum install -y ntpdatentpdate -u ntp.api.bz# 升级内核rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpmyum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -ygrub2-set-default 0"""doneecho $MASTER_IP master$&#123;HOSTNAME_DOMIN&#125; &gt;&gt; /etc/hostshostnamectl set-hostname master$&#123;HOSTNAME_DOMIN&#125;num=1for host in $NODES_IP;do echo $host node$&#123;num&#125;$&#123;HOSTNAME_DOMIN&#125; &gt;&gt; /etc/hosts ssh $host "hostnamectl set-hostname node$&#123;num&#125;$&#123;HOSTNAME_DOMIN&#125;" let num++done for host in $NODES_IP;do scp /etc/hosts root@$host:/etc/hostsdonefor host in $NODES_IP;dossh $host "reboot"done reboot 4.2. 安装脚本install.sh123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139#!/bin/bash########################### SETING ############################################# 设置master节点IP地址（只能写一个）MASTER_IP="192.168.65.101"# 设置node节点IP地址（可以设置多个以空格分割）NODES_IP="192.168.65.103"# 配置kubrnetes版本VERSION="v1.13.2"# pod网络的cidr(默认为flannel)POD_CIDR="10.244.0.0/16"##############################################################################for host in `eval echo "$MASTER_IP $NODES_IP"`;dossh $host """# 安装docker# step 1: 安装必要的一些系统工具yum install -y yum-utils device-mapper-persistent-data lvm2# Step 2: 添加软件源信息yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# Step 3: 更新并安装 Docker-CEyum makecache fastyum -y install docker-ce-18.06.1.ce-3.el7# 取消docker启动后iptables转发限制mkdir -p /lib/systemd/system/docker.service.dcat &gt;/lib/systemd/system/docker.service.d/iptables.conf &lt;&lt; EOF[Service]ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPTEOFmkdir -p /etc/docker/cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; \"registry-mirrors\": [\"https://hub-mirror.c.163.com","https://docker.mirrors.ustc.edu.cn\"], \"max-concurrent-downloads\": 20&#125;EOFsystemctl daemon-reloadsystemctl enable docker.servicesystemctl start docker.service || systemctl restart docker.service# 安装kubeadm，kubectl，kubeletecat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFsetenforce 0yum install -y kubelet kubeadm kubectlsystemctl enable kubelet systemctl start kubelet || systemctl start kubelet# 加载内核模块cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashipvs_modules=\"ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4\"for kernel_module in \$&#123;ipvs_modules&#125;; do /sbin/modinfo -F filename \$&#123;kernel_module&#125; &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe \$&#123;kernel_module&#125; fidoneEOFchmod 755 /etc/sysconfig/modules/ipvs.modules bash /etc/sysconfig/modules/ipvs.moduleslsmod | grep ip_vsyum install -y ipvsadm# 配置内核参数cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confvm.swappiness = 0net.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF#生效配置文件sysctl -p /etc/sysctl.d/k8s.conf"""done# 拉取镜像images=`kubeadm --kubernetes-version=$VERSION config images list|awk -v FS='/' '&#123;print $2&#125;'`for image in `kubeadm --kubernetes-version=$VERSION config images list|awk -v FS='/' '&#123;print $2&#125;'` ;do docker pull mirrorgooglecontainers/$image docker tag mirrorgooglecontainers/$image k8s.gcr.io/$imagedoneimage=`kubeadm --kubernetes-version=$VERSION config images list|awk -v FS='/' '&#123;print $2&#125;'|grep coredns`docker pull coredns/$imagedocker tag coredns/$image k8s.gcr.io/$image# 初始化maserecho """apiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfigurationkubernetesVersion: $VERSIONnetworking: # This CIDR is a Calico default. Substitute or remove for your CNI provider. podSubnet: $POD_CIDR---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationmode: ipvs""" &gt; /etc/kubernetes/kubeadm-config.yamlkubeadm init --config /etc/kubernetes/kubeadm-config.yamlrm -rf ~/.kubemkdir ~/.kube -pcp /etc/kubernetes/admin.conf ~/.kube/curl -fsSL https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml|sed "s@10.244.0.0/16@$&#123;POD_CIDR&#125;@"| kubectl apply -f -# 添加node节点到集群images=""for line in k8s.gcr.io/kube-proxy k8s.gcr.io/coredns k8s.gcr.io/pause;do image=`docker images|grep $line |awk '&#123;print $3&#125;'|xargs` images="$images $image"donedocker save $images -o ~/node_imagesjoin_cmd=`kubeadm token create --print-join-command`for host in $NODES_IP ;do scp ~/node_images root@$host:/root/node_images ssh root@$host "docker load &lt; /root/node_images" ssh root@$host "$join_cmd"done]]></content>
      <categories>
        <category>linux运维</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>linux运维</tag>
        <tag>kubernets</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[glusterfs作为kubernetes后端storageclass]]></title>
    <url>%2F2018%2F12%2F26%2Fglusterfs%E4%BD%9C%E4%B8%BAkubernetes%E5%90%8E%E7%AB%AFstorageclass%2F</url>
    <content type="text"><![CDATA[1.安装glusterfs集群1.1.环境说明3台机器安装 GlusterFS 组成一个 Distributed Replicated Volumes集群 服务清单： 主机名 ip 安装组件 node1 192.168.65.26 glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma heketi heketi-client node2 192.168.65.26 glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma node3 192.168.65.26 glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma 配置/etc/hosts解析 123456127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.65.26 node1192.168.65.27 node2192.168.65.28 node3 1.2.安装glusterfs服务端1.2.1.准备工作关闭iptables和selinux 1.2.2.安装glusterFS服务器每台服务器都安装glusterFS 12yum install centos-release-glusteryum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma 1.2.3.启动12systemctl start glusterdsystemctl enable glusterd 1.2.4.加入 trusted storage pool在任意节点执行 gluster peer probe其它节点（只运行一次） 在node1执行 12gluster peer probe node2gluster peer probe node3 查看节点信息 1gluster peer status 在各节点执行可以看到其它节点信息 1.2.5.创建Volumes（可选）在各节点创建glusterfs存储目录 1mkdir -p /gluster/data&#123;1,2&#125; 1.2.5.1.创建分布式复制卷：1gluster volume create vol1 replica 2 transport tcp node1:/gluster/data1 node2:/gluster/data1 node3:/gluster/data1 node1:/gluster/data2 node2:/gluster/data2 node3:/gluster/data2 GlusterFS 几种volume 模式说明： 一、 Distributed Volumes，默认模式，DHT 1gluster volume create test-volume server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 二、 Replicated Volumes，复制模式，AFR 1gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp2 避免脑裂，加入仲裁 1gluster volume create replica 3 arbiter 1 host1:brick1 host2:brick2 host3:brick3 三、Striped Volumes 1gluster volume create test-volume stripe 2 transport tcp server1:/exp1 server2:/exp2 四、Distributed Striped Volumes，最少需要4台服务器。 1gluster volume create test-volume stripe 4 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 server5:/exp5 server6:/exp6 server7:/exp7 server8:/exp8 五、Distributed Replicated Volumes，最少需要4台服务器。 1gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 六、Distributed Striped Replicated Volumes 1gluster volume create test-volume stripe 2 replica 2 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 server5:/exp5 server6:/exp6 server7:/exp7 server8:/exp8 七、Striped Replicated Volumes 1gluster volume create test-volume stripe 2 replica 2 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 1.2.5.2.启动卷1gluster volume start vol1 1.2.5.3.查看卷1gluster volume info 1.2.5.4.停止卷1gluster volume stop vol1 1.2.5.5.删除卷1gluster volume delete vol1 1.2.安装客户端（可选）在客户端执行 12yum install centos-release-glusteryum -y install glusterfs glusterfs-fuse 创建挂载点 1mkdir /mnt/gluster 将服务器上的逻辑卷file-service挂在到本地/mnt/gluster 1mount -t glusterfs 192.168.65.26:/vol1 /mnt/gluster 查看挂载 1df -h 1.3.安装heketiHeketi用来管理GlusterFS，并提供RESTful API接口供Kubernetes调用。Heketi需要使用裸磁盘（不能格式化文件系统和挂载） 1.3.1.安装软件在node1执行 1yum install heketi heketi-client.x86_64 -y 1.3.2.修改配置1.3.2.1.修改/etc/heketi/heketi.json文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&#123; "_port_comment": "Heketi Server Port Number", "port": "8083", "_use_auth": "Enable JWT authorization. Please enable for deployment", "use_auth": true, "_jwt": "Private keys for access", "jwt": &#123; "_admin": "Admin has access to all APIs", "admin": &#123; "key": "My Secret" &#125;, "_user": "User only has access to /volumes endpoint", "user": &#123; "key": "My Secret" &#125; &#125;, "_glusterfs_comment": "GlusterFS Configuration", "glusterfs": &#123; "_executor_comment": [ "Execute plugin. Possible choices: mock, ssh", "mock: This setting is used for testing and development.", " It will not send commands to any node.", "ssh: This setting will notify Heketi to ssh to the nodes.", " It will need the values in sshexec to be configured.", "kubernetes: Communicate with GlusterFS containers over", " Kubernetes exec api." ], "executor": "ssh", "_sshexec_comment": "SSH username and private key file information", "sshexec": &#123; "keyfile": "/root/.ssh/id_rsa", "user": "root", "port": "22", "fstab": "/etc/fstab" &#125;, "_kubeexec_comment": "Kubernetes configuration", "kubeexec": &#123; "host" :"https://kubernetes.host:8443", "cert" : "/path/to/crt.file", "insecure": false, "user": "kubernetes username", "password": "password for kubernetes user", "namespace": "OpenShift project or Kubernetes namespace", "fstab": "Optional: Specify fstab file on node. Default is /etc/fstab" &#125;, "_db_comment": "Database file name", "db": "/var/lib/heketi/heketi.db", "_loglevel_comment": [ "Set log level. Choices are:", " none, critical, error, warning, info, debug", "Default is warning" ], "loglevel" : "debug" &#125;&#125; 修改以下字段： 端口号：”port”: “8083” 启用认证：”use_auth”: true admin密码：”key”: “ndaX0cDKeuzs” 执行方式：”executor”: “ssh” 私钥文件：”keyfile”: “/root/.ssh/id_rsa” 用户：”user”: “root” 端口号：”port”: “22” 挂载配置文件：”fstab”: “/etc/fstab” 3.2.2.修改/usr/lib/systemd/system/heketi.service文件1234567891011121314[Unit]Description=Heketi Server[Service]Type=simpleWorkingDirectory=/var/lib/heketiUser=rootExecStart=/usr/bin/heketi --config=/etc/heketi/heketi.jsonRestart=on-failureStandardOutput=syslogStandardError=syslog[Install]WantedBy=multi-user.target 修改以下字段： 用户：User=root 1.3.2.3.配置root用户免密登录node1上执行 1234567# 如果已经配置了秘钥请忽略此步骤ssh-keygen -t rsa -f ~/.ssh/id_rsa -N ""# 如果已经免密登录请忽略此步骤ssh-copy-id -i root@node1ssh-copy-id -i root@node2ssh-copy-id -i root@node3 1.3.3.启动服务1234systemctl daemon-reloadsystemctl start heketi glusterfsdsystemctl enable heketi glusterfsdsystemctl status heketi glusterfsd 1.3.4.设置heketi的server信息设置环境变量方便使用heketi-cli工具 123export HEKETI_CLI_SERVER=http://node1:8083export HEKETI_CLI_USER=adminexport HEKETI_CLI_KEY=ndaX0cDKeuzs 1.3.5.验证12345curl $HEKETI_CLI_SERVER/hello ;echo# 或者heketi-cli cluster list# 如没用设置环境变量请使用如下命令heketi-cli --server http://node1:8083 --user admin --secret "ndaX0cDKeuzs" cluster list 1.4.配置节点1.4.1新建 topology.json文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&#123; "clusters": [ &#123; "nodes": [ &#123; "node": &#123; "hostnames": &#123; "manage": [ "node1" ], "storage": [ "192.168.65.26" ] &#125;, "zone": 1 &#125;, "devices": [ "/dev/vdb", "/dev/vdc" ] &#125;, &#123; "node": &#123; "hostnames": &#123; "manage": [ "node2" ], "storage": [ "192.168.65.27" ] &#125;, "zone": 1 &#125;, "devices": [ "/dev/vdb", "/dev/vdc" ] &#125;, &#123; "node": &#123; "hostnames": &#123; "manage": [ "node3" ], "storage": [ "192.168.65.28" ] &#125;, "zone": 1 &#125;, "devices": [ "/dev/vdb", "/dev/vdc" ] &#125; ] &#125; ]&#125; 注：本设置采用两块裸硬盘vdb，vdc 1.4.2.载入配置123heketi-cli topology load --json=topology.json# 如没用设置环境变量请使用如下命令# heketi-cli --server http://node1:8083 --user admin --secret "ndaX0cDKeuzs" topology load --json=topology.json 1.4.3.查看拓扑123heketi-cli topology info# 如没用设置环境变量请使用如下命令heketi-cli --server http://node1:8083 --user admin --secret "ndaX0cDKeuzs" topology info 1.4.4.测试1.4.4.1.创建测试volume1heketi-cli volume create --size=2 1.4.4.2.查看123heketi-cli volume listlsblkcat /etc/fstab 1.4.4.3.删除1heketi-cli delete volume &lt;Id&gt; 2.安装kubernetes参见之前教程 3.配置storageclass3.1.编辑配置清单新建文件glusterfs-storageclass.yaml 1234567891011121314apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: glusterfsprovisioner: kubernetes.io/glusterfsreclaimPolicy: Retainparameters: gidMax: "50000" gidMin: "40000" resturl: http://192.168.65.26:8083 volumetype: replicate:3 restauthenabled: "true" restuser: "admin" restuserkey: "ndaX0cDKeuzs" 应用配置 1kubectl apply -f glusterfs-storageclass.yaml 3.2.创建测试PVC新建文件test-pvc.yaml 123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: test annotations: volume.beta.kubernetes.io/storage-class: "glusterfs"spec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi 应用配置 1kubectl apply -f test-pvc.yaml 3.3.创建测试POD挂载PVC新建文件test-pod.yaml 12345678910111213141516apiVersion: v1kind: Podmetadata: name: test namespace: defaultspec: containers: - name: nginx image: nginx volumeMounts: - name: testvol mountPath: /data volumes: - name: testvol persistentVolumeClaim: claimName: test 应用配置 1kubectl apply -f test-pvc.yaml 3.4.查看123kubectl get pvckubectl get pvkubectl get pod test]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>glusterfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git-flow流程]]></title>
    <url>%2F2018%2F09%2F11%2Fgit-flow%2F</url>
    <content type="text"><![CDATA[1. Git flow 工作流程Git Flow 工作流一共包含五种分支： 1.1. 两个长期分支：1.1.1. 主分支 master：主分支，产品的功能全部实现后，最终在master分支对外发布。 1.1.2. 开发分支 develop：开发分支，基于master分支克隆，产品的编码工作在此分支进行。 ​ 创建Develop分支的命令： 1git checkout -b develop master ​ 将Develop分支发布到Master分支的命令： 1234#切换到Master分支git checkout master# 对Develop分支进行合并git merge --no-ff develop 1.2. 三个临时性分支：1.2.1. 功能（feature）分支：新功能分支，一般一个新功能对应一个分支，功能分支的名字，可以采用 feature-* 的形式命名。 ​ 创建一个功能分支： 1git checkout -b feature-x develop ​ 开发完成后，将功能分支合并到 develop 分支： 12git checkout developgit merge --no-ff feature-x ​ 删除 feature 分支： 1git branch -d feature-x 1.2.2. 预发布（release）分支：发布分支是从 Develop 分支上面分出来的，用于发布测试版本，测试完后最终需要合并进 Develop 和 Master 分支。它的命名，可以采用release-*的形式。 ​ 创建一个预发布分支： 1git checkout -b release-1.2 develop ​ 确认没有问题后，合并到 master 分支： 123git checkout mastergit merge --no-ff release-1.2git tag -a 1.2 # （ 对合并生成的新节点，做一个标签 ） ​ 再合并到 develop 分支： 12git checkout develop git merge --no-ff release-1.2 ​ 最后，删除预发布分支： 1git branch -d release-1.2 1.2.3. 修补bug（fixbug）分支：用于修复线上版本出现的 Bug 时创建的，基于 Master 分支创建。修补结束以后，再合并进 Master 和 Develop 分支。它的命名，可以采用 fixbug-* 的形式。 ​ 创建一个修补 bug 分支： 1git checkout -b fixbug-0.1 master ​ 修补结束后，合并到 master 分支： 123git checkout master git merge --no-ff fixbug-0.1 git tag -a 0.1.1 ​ 再合并到 develop 分支： 12git checkout develop git merge --no-ff fixbug-0.1 ​ 最后，删除”修补 bug 分支”： 1git branch -d fixbug-0.1 2. Git flow 用法2.1. 安装​ MAC OS 1brew install git-flow-avh ​ Linux 1apt-get install git-flow 2.2. 初始化1git flow init 这会创建上述5个分支，你必须回答几个关于分支的命名约定的问题。 建议使用默认值。 2.3. 开发新特性新特性的开发是在feature分支下完成的 ​ 2.3.1. 基于develop分支创建新的分支feature/MYFEATURE分支。 12git flow feature start MYFEATURE# 创建并进入feature/MYFEATURE分支 ​ 2.3.2. 发布新特性 12git flow feature publish MYFEATURE# 将新的特性推送到develop分支 ​ 2.3.3. 完成新特性 12345git flow feature finish MYFEATURE# 将会自动如下三部# 合并 MYFEATURE 分支到 'develop'# 删除这个新特性分支# 切换回 'develop' 分支 2.4. 发布release版本基于develop分支创建，完成后自动打tag、推送到master分支、推送回develop分支 2.4.1. 基于develop分支在release分支之下创建新分支release/RELEASE 12git flow release start RELEASE # 创建并进入release/RELEASE分支 ​ 2.4.2. 完成 release 版本 12345git flow release finish RELEASE# 归并 release 分支到 'master' 分支# 用 release 分支名打 Tag# 归并 release 分支到 'develop'# 移除 release 分支 2.5. 紧急修复 急修复来自这样的需求：生产环境的版本处于一个不预期状态，需要立即修正。 有可能是需要修正 master 分支上某个 TAG 标记的生产版本。 基于master分支创建，完成后自动打tag、推送回master分支、推送到develop分支 2.5.1. 基于master分支在hotfix分支之下创建新分支hotfix/VERSION 12git flow hotfix start VERSION [BASENAME]# [BASENAME]为finish release时填写的版本号 ​ 2.5.2. 完成紧急修复 12345git flow hotfix finish VERSION# 归并 hotfix 分支到 'master' 分支# 用 hotfix 分支名打 Tag# 归并 hotfix 分支到 'develop'# 移除 hotfix 分支 3. Git flow 命令 第一段（命令） 第二段（分支） 第三段（操作） 第四段（名字） git flow init start NAME feature finish release publish hotfix pull]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch实现与ldap对接]]></title>
    <url>%2F2018%2F08%2F27%2Felasticsearch%E5%AE%9E%E7%8E%B0ldap%E7%94%A8%E6%88%B7%E5%AF%B9%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[一、 elasticsearch安装1.1 准备安装环境及安装包环境搭建： ​ 操作系统：debian9 ​ 硬件资源：4cores；8Gb；1台 JDK： ​ openjdk1.8 1apt-get install openjdk-8-jdk ELK版本：5.6.8 ​ 下载路径：https://www.elastic.co/downloads/past-releases ​ elasticsearch、logstash、kibana、filebeat版本必须保持一致 ​ elasticsearch负责存储数据，并向外提供RESTful接口，本身支持集群 ​ logstash负责整理数据，并将数据序列化发往elasticsearch ​ kibana负责数据展示，并可作为elasticsearch的dashboard提供GUI接口 ​ filebeat轻量化的日志收集器，可将日志发往logstash、redis、elasticsearch等 123456mkdir elkcd elkwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.8.debwget https://artifacts.elastic.co/downloads/logstash/logstash-5.6.8.debwget https://artifacts.elastic.co/downloads/kibana/kibana-5.6.8-amd64.debwget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-5.6.8-amd64.deb 1.2 安装elasticsearch安装： 1dpkg -i elasticsearch-5.6.8.deb 配置： ​ 编辑配置文件“/etc/elasticsearch/elasticsearch.yml”，更改以下几项 123456789101112cluster.name: myelastic #集群名称node.name: node-1 #节点名称network.host: 0.0.0.0http.port: 9200path.data: /data/elasticsearch/data #数据存放路径path.logs: /data/elasticsearch/log #日志存放路径discovery.zen.ping.unicast.hosts: #集群成员- "IP1"- "IP2"- "IP3"discovery.zen.minimum_master_nodes: 2 #最少主节点数量 ​ 集群数量可根据需求调整 ​ 编辑配置文件“/etc/elasticsearch/jvm.options”，更改以下几项 12-Xms3g #初始化jvm堆内存-Xmx3g #最大jvm可用内存 ​ 上述两个值必须一致，否则无法启动 创建数据目录： 12mkdir -p /data/elasticsearch/data /data/elasticsearch/logchown elasticsearch.elasticsearch /data/elasticsearch/&#123;data,log&#125; 启动elasticsearch： 123systemctl enable elasticsearch.servicesystemctl restart elasticsearch.servicesystemctl status elasticsearch.service 1.3 安装logstash安装： 1dpkg -i logstash-5.6.8.deb 配置： ​ 编辑配置文件“/etc/logstash/logstash.yml”，更改以下几项 12path.data: /data/logstash/data #数据存放路径path.logs: /data/logstash/log #日志存放路径 ​ 编辑配置文件“/etc/logstash/jvm.options”，更改以下几项 12-Xms2g #初始化jvm堆内存-Xmx2g #最大jvm可用内存 ​ 创建配置文件“/etc/logstash/conf.d/app.conf” 1234567891011121314151617181920212223242526272829input &#123; beats &#123; port =&gt; 5044 &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; "message" =&gt; "%&#123;HTTPD_COMBINEDLOG&#125;" &#125; remove_field =&gt; "message" &#125; date &#123; match =&gt; ["timestamp","dd/MMM/YYYY:H:m:s Z"] &#125; geoip &#123; source =&gt; "clientip" target =&gt; "geoip" database =&gt; "/etc/logstash/maxmind/GeoLite2-City.mmdb" &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [ "http://IP1:9200","http://IP2:9200","http://IP3:9200" ] index =&gt; "logstash-%&#123;+YYYY.MM.dd&#125;" action =&gt; "index" document_type =&gt; "apache_logs" &#125;&#125; 下载geoip数据库 1234wget http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.tar.gztar xf GeoLite2-City.tar.gz -C /etc/logstash/rm -rf /etc/logstash/maxmindmv /etc/logstash/GeoLite2-City_20* /etc/logstash/maxmind 创建数据目录： 12mkdir -p /data/logstash/data /data/logstash/logchown logstash.logstash /data/logstash/&#123;data,log&#125; 启动logstash： 123systemctl enable logstash.servicesystemctl restart logstash.servicesystemctl status logstash.service 1.4 安装kibana安装： 1dpkg -i kibana-5.6.8-amd64.deb 配置： ​ 编辑配置文件“/etc/kibana/kibana.yml”，更改以下几项 123server.port: 5601 #监听端口server.host: 0.0.0.0 #监听地址elasticsearch.url: "http://IP1:9200" #任意一个elasticsearchIP 启动kibana： 123systemctl enable kibana.servicesystemctl restart kibana.servicesystemctl status kibana.service 1.5 安装filebeat安装： 1dpkg -i filebeat-5.6.8-amd64.deb 配置： ​ 编辑配置文件“/etc/filebeat/filebeat.yml”，更改以下几项 123456- input_type: log paths: - /var/log/apache2/access.log* #收集日志路径output.logstash: hosts: ["LOGSTASH_IP:5044"] 启动： 123systemctl enable filebeat.servicesystemctl restart filebeat.servicesystemctl status filebeat.service 二、配置elasticsearch与ldap对接2.1 安装x-pack插件下载对应版本x-pack插件： 12cd /root/elkwget https://artifacts.elastic.co/downloads/packs/x-pack/x-pack-5.6.8.zip elasticsearch安装x-pack插件： 123systemctl stop elasticsearch.service/usr/share/elasticsearch/bin/elasticsearch-plugin install file:///root/elk/x-pack-5.6.8.zipsystemctl start elasticsearch.service kibana安装x-pack插件： 123systemctl stop kibana.service/usr/share/kibana/bin/kibana-plugin install file:///root/elk/x-pack-5.6.8.zipsystemctl start kibana.service 系统会默认生成三个账号elastic、kibana、logstash_system 其中elastic用户拥有最高权限，密码默认为”changeme“ 以上三个账号密码均可在kibana中修改，修改密码后需要更改“/etc/kibana/kikana.yml”配置文件 2.2 配置elasticsearch编辑配置文件“”，在最后添加如下代码 123456789101112131415161718xpack: security: authc: realms: ldap1: type: ldap order: 0 url: "ldap://10.2.0.1" #ldap服务器地址 bind_dn: "cn=admin,dc=ffq, dc=com" #管理员dn bind_password: "password" #管理员密码 user_search: base_dn: "dc=ffq,dc=com" #用户搜索范围 attribute: cn group_search: base_dn: "dc=ffq,dc=com" files: role_mapping: "/etc/elasticsearch/x-pack/role_mapping.yml" #角色绑定配置文件 unmapped_groups_as_roles: false 编辑配置文件“/etc/elasticsearch/x-pack/role_mapping.yml”，添加“用户”与“角色”绑定 ​ 配置组绑定无效，原因未知，可启动后使用命令配置 12345superuser: #角色role - "cn=admin,dc=ffq,dc=com" #用户dnwatcher_admin: - "cn=admin,dc=ffq,dc=com" - "cn=fanfengqiang,dc=ffq,dc=com" 重启elasticsearch 1systemctl start kibana.service 添加用户角色绑定 1234567891011curl -X POST "localhost:9200/_xpack/security/role_mapping/users" -u elastic:changeme -H 'Content-Type: application/json' -d'&#123; "roles": [ "superuser" ], "enabled": true, "rules": &#123; "field" : &#123; "dn" : "*,ou=Users,dc=ffq,dc=com" &#125; &#125;&#125;'curl "localhost:9200/_xpack/security/role_mapping?pretty" -u elastic:changeme 2.3 测试12curl http://127.0.0.1:9200/_cat?pretty -u ldapuser:passwordcurl http://127.0.0.1:9200/_cat/indices?pretty -u ldapuser:password 若不报400、401或403错误则配置成功 ​]]></content>
      <categories>
        <category>linux运维</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>linux运维</tag>
        <tag>elasticsearch</tag>
        <tag>ldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins]]></title>
    <url>%2F2018%2F08%2F23%2Fjenkins%2F</url>
    <content type="text"><![CDATA[1 安装1.1 安装JDK上传jdk二进制包“jdk-8u111-linux-x64.rpm ” 1yum install ./jdk-8u111-linux-x64.rpm 注意： jdk版本必须大于等于1.8 1.2 安装jenkins下载RPM包“jenkins-2.121.2-1.1.noarch.rpm” 1yum install jenkins-2.121.2-1.1.noarch.rpm 1.3 修改配置文件编辑“/etc/sysconfig/jenkins” 123JENKINS_HOME=“/data“ #数据目录，使用高IO大容量磁盘JENKINS_USER=“jenkins“ #启动用户JENKINS_PORT=“8080“ #启动端口 1.4 启动jenkins123chkconfig jenkins on/etc/init.d/jenkins startss -ntlp |grep 8080]]></content>
      <categories>
        <category>linux运维</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>linux运维</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitlab安装、备份与恢复]]></title>
    <url>%2F2018%2F08%2F23%2Fgitlab%E5%AE%89%E8%A3%85%E3%80%81%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[1 安装1.1 安装JDK上传jdk二进制包“jdk-7u79-linux-x64.tar.gz” 12345678910111213tar xvf jdk-7u79-linux-x64.tar.gz -C /usr/local/cd /usr/local/ln -s jdk1.7.0_79/ jdkcat &gt; /etc/profile.d/jdk.sh &lt;&lt; EOF#!/bin/bash##********************************************************************#Description：JAVA_HOME=/usr/local/jdkPATH="\$JAVA_HOME/bin:\$PATH"CLASSPATH=.:\$JAVA_HOME/lib/dt.jar:\$JAVA_HOME/lib/tools.jarEOFsource /etc/profile.d/jdk.sh 1.2 安装gitlab12wget https://packages.gitlab.com/gitlab/gitlab-ce/packages/el/7/gitlab-ce-8.13.5-ce.0.el7.x86_64.rpmyum install gitlab-ce-8.13.5-ce.0.el7.x86_64.rpm 1.3 配置gitlab编辑配置文件脚本“/etc/gitlab/gitlab.rb”，修改以下几项 123456789101112external_url 'http://10.2.2.1' #服务器IP地址gitlab_rails['smtp_enable'] = true gitlab_rails['smtp_address'] = "smtp.163.com"gitlab_rails['smtp_port'] = 25gitlab_rails['smtp_user_name'] = "XXXXXX@163.com" # 发件人邮箱gitlab_rails['smtp_password'] = "YOURPASSWORD"gitlab_rails['smtp_domain'] = "163.com"gitlab_rails['smtp_authentication'] = :logingitlab_rails['smtp_enable_starttls_auto'] = truegitlab_rails['smtp_tls'] = falsegitlab_rails['gitlab_email_from'] = "XXXXXXX@163.com"user["git_user_email"] = "XXXXXXX@163.com" # 收件人邮箱 生成配置文件 1gitlab-ctl reconfigure 1.4 启动12gitlab-ctl startgitlab-ctl status 2 备份2.1 修改配置vim /etc/gitlab/gitlab.rb 修改以下几项 1234gitlab_rails['manage_backup_path'] = truegitlab_rails['backup_path'] = "/data/gitlab/backups" //gitlab备份目录gitlab_rails['backup_archive_permissions'] = 0644 //生成的备份文件权限gitlab_rails['backup_keep_time'] = 7776000 //备份保留天数为3个月（即90天，这里是7776000秒） 创建备份目录 123mkdir -p /data/gitlab/backupschown -R git.git /data/gitlabchmod -R 700 /data/gitlab/ 重新生成配置文件 1gitlab-ctl reconfigure 2.2 执行备份操作生成备份 1gitlab-rake gitlab:backup:create 3 恢复3.1 停止相关服务123gitlab-ctl stop unicorngitlab-ctl stop sidekiqgitlab-ctl status 3.2 执行恢复123cd /data/gitlab/backupsllgitlab-rake gitlab:backup:restore BACKUP=XXXXXXXX 注意：XXXXXXX中不带“_gitlab_backup.tar”后缀 3.3 启动gitlab12gitlab-ctl startgitlab-ctl status 3.4 check检查数据1gitlab-rake gitlab:check SANITIZE=true 注意：Gitlab迁移与恢复一样，但是要求两个GitLab版本号一致]]></content>
      <categories>
        <category>linux运维</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>linux运维</tag>
        <tag>gitlab</tag>
      </tags>
  </entry>
</search>
